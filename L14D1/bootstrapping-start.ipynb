{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "975b67b5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 14 Day 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 6 December 2022, with material from Willie Wheeler</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba49d443",
   "metadata": {},
   "source": [
    "# Boostrapping Lab\n",
    "I told you that there was a way to build standard deviations using classical techniques rather than Bayesian simulations. Today I want to show you how to do it. It's called **bootstrapping**.\n",
    "\n",
    "Bootstrapping is any test or metric that uses random sampling with replacement (e.g. mimicking the sampling process), and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates.[1][2] This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.[3][4]\n",
    "\n",
    "Bootstrapping estimates the properties of an estimand (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution function of the observed data. In the case where a set of observations can be assumed to be from an independent and identically distributed population, this can be implemented by constructing a number of resamples with replacement, of the observed data set (and of equal size to the observed data set).\n",
    "\n",
    "It may also be used for constructing hypothesis tests. It is often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors.\n",
    "\n",
    "Let's warm up with using an MC simulation to estimate a probability\n",
    "\n",
    "# 1. Monte Carlo sim to estimate a probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eea7e7",
   "metadata": {},
   "source": [
    "### Hypothetical Problem\n",
    "The logarithms of weights (in pounds) of men in the United States are approximately normally distributed with mean 5.13 and standard deviation 0.17; women’s log weights are approximately normally distributed with mean 4.96 and standard deviation 0.20. Suppose 10 adults selected at random step on an elevator with a capacity of 1750 pounds. What is the probability that their total weight exceeds this limit?\n",
    "\n",
    "We need to estimate the probability that the 10 randomly selected adults together weigh more than 1,750 pounds. An individual man’s weight follows a lognormal distribution, and an individual woman’s weight follows a different lognormal distribution. If the problem were to estimate the probability that a single randomly selected adult’s weight exceeds some value — maybe 180 pounds — this would be easy enough to do analytically. But the part where we choose 10 adults at random makes it harder.\n",
    "\n",
    "Write a simple computer simulation that we can run a bunch of times. Each run will generate a single total weight (the random variable under study) by generating a random sets of adults, each of whom has a randomly generated weight according to the corresponding lognormal distribution. The set of runs gives us a distribution. Then we’ll just look at the proportion of runs where the total weight exceeds 1,750 pounds, and that’s our estimate.\n",
    "\n",
    "There’s one unstated parameter we need to make explicit, which is the proportion of men versus women in the United States. According to the book, the proportions are men = 0.48 and women = 0.52."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b634ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_male = 0.48\n",
    "man_log_weight_mean = 5.13\n",
    "man_log_weight_std = 0.17\n",
    "woman_log_weight_mean = 4.96\n",
    "woman_log_weight_std = 0.20\n",
    "num_adults = 10\n",
    "weight_limit = 1750"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186196b1",
   "metadata": {},
   "source": [
    "Write a Monte Carlo simulation that yields the distribution of the total weight of 10 adults using the following distributions:\n",
    "```\n",
    "np.random.binomial()\n",
    "```\n",
    "\n",
    "and\n",
    "```\n",
    "np.random.normal\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f283f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819e85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_m = np.random.binomial(man_log_weight_mean, man_log_weight_std, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3653cd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 0, 1, 0, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a247418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_mn=np.random.normal(man_log_weight_mean, man_log_weight_std, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ed4efbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male = np.random.binomial(1, pr_male)\n",
    "male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c5f6269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 0, 1, 0, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a4c9b",
   "metadata": {},
   "source": [
    "The solution is here below. Only click below *after* you finished writing your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce65329",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "    import numpy as np\n",
    "num_sims = 1000\n",
    "total_weight = []\n",
    "for s in range(num_sims):\n",
    "    male = np.random.binomial(1, pr_male)\n",
    "    if male == 1:\n",
    "        logweight = np.random.normal(man_log_weight_mean, man_log_weight_std, num_adults)\n",
    "    else:\n",
    "        logweight = np.random.normal(woman_log_weight_mean, woman_log_weight_std, num_adults)\n",
    "    weight = np.exp(logweight)\n",
    "    total_weight.append(sum(weight))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dad246",
   "metadata": {},
   "source": [
    "Distribution of total weight of 10 adults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(total_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848663b",
   "metadata": {},
   "source": [
    "From the above we can see that a majority of the runs result in totals less than 1,750 pounds. Looks like maybe 15% are over the limit. Calculate a precise estimate here below. Please write your own code, and only *after* you have a result, then click on the cell further below to double-check your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744ad1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a27750a",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "    len(list(filter(lambda w: w > 1750, total_weight))) / num_sims\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dfa23b",
   "metadata": {},
   "source": [
    "The answer for my 1,000 runs is 18%, so that’s my estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f334b",
   "metadata": {},
   "source": [
    "# 2. Bootstrapping What-if analyses\n",
    "Say we own a web service and we want to improve the processing time for one of the web service endpoints. \n",
    "\n",
    "Behind the scenes, request processing happens in stages, each with its own duration, and these sum to produce an overall duration. \n",
    "\n",
    "We’d like to conduct experiments that explore the impact of component distribution changes on the overall distribution. Such experiments would allow us to answer questions like:\n",
    "\n",
    "- How can we reduce the 95th precentile of the total duration by 10%?\n",
    "- How can we reduce standard deviation of the total duration by 20%?\n",
    "- We want to make a change to one of the stages that will increase its both the mean and the standard deviation of the duration by such-and-such amounts. What will be the impact on the 50th percentile and the 90th precentile of the total duration?\n",
    "\n",
    "We can use probability simulations to conduct analyses of this sort. The basic idea is to create a simulation that generates random values at each stage according to underlying component distributions, and then sums them to get a total. Run the simulation a large number of times to generate histograms from which we can extract corresponding probability distributions.\n",
    "\n",
    "First, we have the simulation itself. For the sake of this simulation, assume three lognormal component distributions. \n",
    "\n",
    "The lognormal distribution takes a mean and standard deviation of the random variable’s natural log. So here we take a list `m` of three means, a list `sd` of three standard deviations, and a count `n` of how many simulation runs we want to conduct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70929b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534c927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "n = 1000\n",
    "xrange = [0, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stages(s0, s1, s2, total):\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(12, 2), sharex=True, tight_layout=True)\n",
    "\n",
    "    ax[0].hist(s0, bins=20, range=xrange)\n",
    "    ax[0].set_title(\"S0 duration\")\n",
    "\n",
    "    ax[1].hist(s1, bins=20, range=xrange)\n",
    "    ax[1].set_title(\"S1 duration\")\n",
    "\n",
    "    ax[2].hist(s2, bins=20, range=xrange)\n",
    "    ax[2].set_title(\"S2 duration\")\n",
    "\n",
    "    ax[3].hist(total, bins=20, range=xrange)\n",
    "    ax[3].set_title(\"Total duration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c364280",
   "metadata": {},
   "source": [
    "Copy paste the simulatin in the cell below:\n",
    "```\n",
    "def run_sim(m, sd, n):\n",
    "    s0 = rng.lognormal(m[0], sd[0], n)\n",
    "    s1 = rng.lognormal(m[1], sd[1], n)\n",
    "    s2 = rng.lognormal(m[2], sd[2], n)\n",
    "    total = s0 + s1 + s2\n",
    "    plot_stages(s0, s1, s2, total)\n",
    "    print(f\"Total p50, p95, p99={np.quantile(total, [0.5, 0.95, 0.99])}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adde62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40330482",
   "metadata": {},
   "source": [
    "##  Baseline\n",
    "Suppose we have the following baselines for `s0`, ``s1`, and `s2`:\n",
    "```\n",
    "base_mean = [2.5, 3.1, 2.1]\n",
    "base_sd = [0.3, 0.4, 0.5]\n",
    "```\n",
    "\n",
    "Obtain the histograms of the `S` components and the total duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10022465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7258b4a",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "run_sim(base_mean, base_sd, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6002c1",
   "metadata": {},
   "source": [
    "## Improved\n",
    "After that, we can do any number of what-if analyses. Say for instance we want to drive the total 95th percentile down from 65.77 ms to something under 50 ms. \n",
    "\n",
    "Would it be sufficient to improve the S1 mean log duration from 3.1 to 2.5?\n",
    "\n",
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4eda3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa2d2bc3",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "imp_mean = [2.5, 2.5, 2.1]\n",
    "imp_sd = base_sd\n",
    "run_sim(imp_mean, imp_sd, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3332df85",
   "metadata": {},
   "source": [
    "If you look carefully at the charts, you will be able to see that the S1 distribution is both lower and tighter (remember that we’re dealing with logarithms here) in the improved system as compared to the baseline system, and so the total distribution is similarly improved. The new 95th percentile is 50.58 ms, which doesn’t quite reach our goal, but is very close.\n",
    "\n",
    ">**Note**: We consider percentile statistics, but obviously we can compare means, standard deviations, ranges or whatever else we find useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79f6b1c",
   "metadata": {},
   "source": [
    "## Sampling from an empirical distribution using the bootstrap algorithm\n",
    "The following dataset contains duration data for 20,000 requests to a given web service endpoint. For each request, the endpoint queries a database, processes the result set, and returns a response. Accordingly, each request has a query duration, a processing duration, and total duration that sums the two.\n",
    "\n",
    "One important point is that each request is either *small* or *large*, though the dataset does not include these labels. *Small* and *large* refer to the size of the query result set, not to the number of requests. There are 15,000 small requests and 5,000 large requests.\n",
    "\n",
    "Here’s functionality for plotting histograms for the query durations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a25e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_trunc(data):\n",
    "    zeros = np.zeros(data.shape[0])\n",
    "    return np.maximum(zeros, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25539eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_quantiles(data_ctrl, data_exp):\n",
    "    q = [0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "    q_ctrl = np.quantile(data_ctrl, q)\n",
    "    q_exp = np.quantile(data_exp, q)\n",
    "    print(\"Baseline        Experiment\")\n",
    "    print(f\"p25={q_ctrl[0]:6.2f}      p25={q_exp[0]:6.2f}\")\n",
    "    print(f\"p50={q_ctrl[1]:6.2f}      p50={q_exp[1]:6.2f}\")\n",
    "    print(f\"p75={q_ctrl[2]:6.2f}      p75={q_exp[2]:6.2f}\")\n",
    "    print(f\"p90={q_ctrl[3]:6.2f}      p90={q_exp[3]:6.2f}\")\n",
    "    print(f\"p95={q_ctrl[4]:6.2f}      p95={q_exp[4]:6.2f}\")\n",
    "    print(f\"p99={q_ctrl[5]:6.2f}      p99={q_exp[5]:6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e93d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, title, plot_range):\n",
    "    plt.hist(data, bins=100, range=plot_range)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695dcf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sim_vs_observed(data_obs, title_obs, data_sim, title_sim, plot_range):\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, tight_layout=True, sharex=True, sharey=False, figsize=(8, 5))\n",
    "\n",
    "    ax[0].hist(data_obs, bins=100, range=plot_range)\n",
    "    ax[0].set_title(title_obs)\n",
    "    ax[0].set_ylabel(\"Count\")\n",
    "\n",
    "    ax[1].hist(data_sim, bins=100, range=plot_range)\n",
    "    ax[1].set_title(title_sim)\n",
    "    ax[1].set_xlabel(\"Duration (ms)\")\n",
    "    ax[1].set_ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6204114",
   "metadata": {},
   "source": [
    "### Generate observations\n",
    "Our hypothetical process is a web service endpoint that queries a database for a list of records, does some processing on each, and then returns the result of that processing to the user. Both query time and processing time are roughly linear in the size of the record list.\n",
    "\n",
    "There are two request classes: a *small* class and a *large* class. *Small* and *large* refer to the number of records returned rather than the number of requests. Indeed we will have more small requests than large requests.\n",
    "\n",
    "Even though we're manufacturing this data, we'll treat it as actual observations for the purposes of this notebook. That is, we will treat the manufactured data as the simulation target; it's not itself the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts for small vs large requests\n",
    "n_total = 20000\n",
    "n_small = 15000\n",
    "n_large = n_total - n_small\n",
    "\n",
    "query_err_small = rng.normal(0.0, 3.0, n_small)\n",
    "query_err_large = rng.normal(0.0, 3.0, n_large)\n",
    "query_dur_small = left_trunc(65.3 + rng.lognormal(1.6, 1.0, n_small) + query_err_small)\n",
    "query_dur_large = left_trunc(85.2 + rng.lognormal(1.6, 1.0, n_large) + query_err_large)\n",
    "query_dur = np.hstack((query_dur_small, query_dur_large))\n",
    "\n",
    "cpu_err_small = rng.normal(0.0, 3.0, n_small)\n",
    "cpu_err_large = rng.normal(0.0, 3.0, n_large)\n",
    "cpu_dur_small = left_trunc(10.0 + 0.3 * query_dur_small + cpu_err_small)\n",
    "cpu_dur_large = left_trunc(38.0 + 0.3 * query_dur_large + cpu_err_large)\n",
    "cpu_dur = np.hstack((cpu_dur_small, cpu_dur_large))\n",
    "\n",
    "total_dur = query_dur + cpu_dur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c9752",
   "metadata": {},
   "source": [
    "Let's plot the query abd CPU durations, as well as the total duration:\n",
    "```\n",
    "plot_data(query_dur, \"Query duration (ms)\", [0, 300])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7dd12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9831699",
   "metadata": {},
   "source": [
    "```\n",
    "plot_data(cpu_dur, \"CPU duration (ms)\", [0, 300])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85f4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eaf2e50",
   "metadata": {},
   "source": [
    "```\n",
    "plot_data(total_dur, \"Total duration (ms)\", [0, 300])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afcf430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22d8caf1",
   "metadata": {},
   "source": [
    "In all three histograms, it’s easy to see the two request classes.\n",
    "\n",
    "There’s a second important point about our dataset. The query duration and CPU duration are **correlated**. Basically, requests involving larger result sets take longer for both querying and processing. Here’s a scatterplot that shows not only the correlation but the two request classes:\n",
    "```\n",
    "import random\n",
    "\n",
    "i = random.sample(range(0, n_total), 1000)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"CPU duration vs query duration\")\n",
    "plt.scatter(x=query_dur[i], y=cpu_dur[i], s=0.1)\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(0, 150)\n",
    "plt.xlabel(\"Query duration (ms)\")\n",
    "plt.ylabel(\"CPU duration (ms)\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36166590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16f5d4db",
   "metadata": {},
   "source": [
    "Our goal is to create a simulation that allows us to manipulate the query and processing duration distributions and see the impact on the total duration distribution. To do this, we need to have a way to sample from the two input distributions, and then we need a way to combine them into the output distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f2c30c",
   "metadata": {},
   "source": [
    "## Bootstrapping: Sampling from an empirical distribution\n",
    "We can use something called **bootstrap sampling** to get new samples, even in the face of correlated inputs. \n",
    "\n",
    "Bootstrapping is normally for estimating sampling distribution statistics, but here we’re using it to generate a dataset to drive our simulation. The idea is to select `n` rows from the dataset at random and with replacement. Because we select the entire row, we preserve any relationships that might exist between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29eade0",
   "metadata": {},
   "source": [
    "```\n",
    "n_sample = 20000\n",
    "i_sample = random.choices(range(0, n_total), k=n_sample)\n",
    "query_sample = query_dur[i_sample]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e9fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70a0942c",
   "metadata": {},
   "source": [
    "```\n",
    "plot_sim_vs_observed(\n",
    "    query_dur, \"Query duration, observed\",\n",
    "    query_sample, \"Query duration, sampled\",\n",
    "    [0, 300])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f48023e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "747b5a88",
   "metadata": {},
   "source": [
    "```\n",
    "dump_quantiles(query_dur, query_sample)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d869162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78cb875d",
   "metadata": {},
   "source": [
    "We drew 20,000 records (again, with replacement) to keep the shape and smoothness consistent between the original and the sample. We can see that the match is very good.\n",
    "\n",
    "Note that this technique won’t always work well. For example, it wouldn’t work well for sparse, high-dimensional data. \n",
    "\n",
    "Now we know how to generate new samples from a given empirical distribution. Let’s apply this knowledge to what-if simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244bb470",
   "metadata": {},
   "source": [
    "## What-if analysis\n",
    "For our what-if analysis, we’d like to understand the impact of making improvements to query durations, CPU durations, or both on the overall duration. That way we can identify the options available to bring the overall duration down to meet a given service level objective (SLO).\n",
    "\n",
    "To do this, we have to be able to sample from the component distributions, which we now know how to do. Then we sum the component durations to get the overall duration.\n",
    "\n",
    "First, let’s define a function to allow us to run the simulation:\n",
    "```\n",
    "def run_sim(query_data, cpu_data, n, k_query=1.0, k_cpu=1.0):\n",
    "    i_sample = random.choices(range(0, n_total), k=n)\n",
    "    return k_query * query_data[i_sample] + k_cpu * cpu_data[i_sample]\n",
    "\n",
    "n_sim = 20000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f508c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f14c7872",
   "metadata": {},
   "source": [
    "In the function above,\n",
    "\n",
    "- query_data and cpu_data are arrays containing query and CPU data,\n",
    "- n is the number of simulation runs,\n",
    "- k_query and k_cpu are multipliers on the query and CPU components.\n",
    "\n",
    "Now we have a simulation that we hope will serve as the basis for doing some what-if experiments. First let’s verify its correctness by using it to generate something like the baseline duration data.\n",
    "\n",
    "### Verify the simulation\n",
    "First we want to make sure that the function generates something reasonable. So we’ll run it with the default k_query and k_cpu and see how it looks compared to the observed total duration.\n",
    "```\n",
    "sim_sample_baseline = run_sim(query_dur, cpu_dur, n_sim)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45d433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f712562",
   "metadata": {},
   "source": [
    "```\n",
    "plot_sim_vs_observed(\n",
    "    total_dur, \"Total duration, observed\",\n",
    "    sim_sample_baseline, \"Total duration, simulated\",\n",
    "    [0, 300])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4208a2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36c4cac5",
   "metadata": {},
   "source": [
    "```\n",
    "dump_quantiles(total_dur, sim_sample_baseline)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866a1d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad5dd918",
   "metadata": {},
   "source": [
    "The generated sample is nearly identical to the original, both qualitatively and quantitatively. This gives us confidence that we can use the components to generate the overall duration. So now let’s do some what-if experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5623d",
   "metadata": {},
   "source": [
    "## Experiment #1: faster queries\n",
    "What if we make queries 20% faster?\n",
    "```\n",
    "sim_sample_whatif_query = run_sim(query_dur, cpu_dur, n_sim, k_query=0.8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f761f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dffb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_vs_observed(\n",
    "    total_dur, \"Total duration, observed\",\n",
    "    sim_sample_whatif_query, \"Total duration, simulated - faster queries\",\n",
    "    [0, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_quantiles(total_dur, sim_sample_whatif_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b3400",
   "metadata": {},
   "source": [
    "## Faster CPU\n",
    "What if we make CPU 20% faster?\n",
    "```\n",
    "sim_sample_whatif_cpu = run_sim(query_dur, cpu_dur, n_sim, k_cpu=0.8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef4f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73275257",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_vs_observed(\n",
    "    total_dur, \"Total duration, observed\",\n",
    "    sim_sample_whatif_cpu, \"Total duration, simulated - faster CPU\",\n",
    "    [0, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28973a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_quantiles(total_dur, sim_sample_whatif_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423af7bd",
   "metadata": {},
   "source": [
    "## Faster queries *and* CPU\n",
    "What if we make queries and CPU 20% faster?\n",
    "```\n",
    "sim_sample_whatif_both = run_sim(query_dur, cpu_dur, n_sim, k_query=0.8, k_cpu=0.8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b776209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6414a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_vs_observed(\n",
    "    total_dur, \"Total duration, observed\",\n",
    "    sim_sample_whatif_both, \"Total duration, simulated - faster queries and CPU\",\n",
    "    [0, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e3b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_quantiles(total_dur, sim_sample_whatif_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c6dfe",
   "metadata": {},
   "source": [
    "## Experiment #4: slower queries\n",
    "What if we make queries 30% slower?\n",
    "\n",
    "This one may seem a little funny, but consider the following use case: We need to make a change to the queries that will make them take longer, and we need to know whether the impact on the overall duration still meets our service level objective.\n",
    "```\n",
    "sim_sample_whatif_query2 = run_sim(query_dur, cpu_dur, n_sim, k_query=1.3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500ecf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ac79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sim_vs_observed(\n",
    "    total_dur, \"Total duration, observed\",\n",
    "    sim_sample_whatif_query2, \"Total duration, simulated - slower queries\",\n",
    "    [0, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c873fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_quantiles(total_dur, sim_sample_whatif_query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7b8167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%watermark` not found.\n"
     ]
    }
   ],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5fbe8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The approach for building probability simulations for response time data was to decompose an overall response time into component response times, randomly generate the component response times according to an appropriate probability distribution, and then add them up to get the overall response time and corresponding distribution of response times. This allows us to run “what-if” scenarios, such as “what would happen to the overall response time if we were to speed up the database queries by 20%?”\n",
    " \n",
    "From the examples above, we can see that sampling from empirical distributions gives us the ability to build simulations that incorporate complex distributions of the sort that often occur in the real world. We can then use these simulations to run experiments that explore the distributional relationship between system inputs and outputs.\n",
    "\n",
    "Something interesting that can happen is that the component response times can be correlated with one another. This isn’t a big surprise: if we have a web request that involves querying a database, larger result sets will often drive longer query times and result processing times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aa1f54",
   "metadata": {},
   "source": [
    "# Homework\n",
    "Use **Bayesian estimation** to conduct the same analysis using a **Gaussian Mixture Model**.\n",
    "\n",
    "Note: No more homework for next week. Work on your final project instead!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
