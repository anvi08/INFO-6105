{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 13 Day 2</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 8 December 2022</div>\n",
    "\n",
    "Let's wrap our semester with a notebook about designing a statistical experiment. What are all the factors to consider? How to compare frequentist and Bayesian approaches?\n",
    "\n",
    "# Designing a statistical experiment: A-B testing in depth\n",
    "\n",
    "A student of mine interviewed for Amazon and told me they told her to study A/B tests for her next interview. So I thought I would do a lecture with an A/B test notebook with you.\n",
    "\n",
    "We already did A/B tests with an IQ pill and romantic sloths, but let's do it again in depth, where we learn *everything* there is to learn about A/B tests and we also contrast a frequentist with a Bayesian approach.\n",
    "\n",
    "An A/B test is a simple *controlled experiment*. Let’s say we want to learn if a new UI is good for users.\n",
    "\n",
    "To run the experiment, we take a subset of users or user sessions, usually a simple random sample, and then use random assignment to evenly split that sample into two groups. Group `A` often called the **control group**, continues to receive the old UI, while Group `B`, often called the **treatment group**. Group B receives the new UI.\n",
    "\n",
    "We wait, and we then compare the values of a variety of metrics from Group A to those from Group B. Some metrics will be specific to the given hypothesis. For a UI experiment, we’ll look at engagement with different variants of the new feature. For an experiment that aims to enage the users more, we’ll measure if members are using the app a bit more. In other types of experiments, we might focus on more technical metrics, such as the time it takes the app to load.\n",
    "\n",
    "There’s a lot of statistics involved as well — how large a difference is considered significant? How many members do we need in a test in order to detect an effect of a given magnitude? How do we most efficiently analyze the data?\n",
    "\n",
    "Because we create our control and treatment groups using random assignment, we can ensure that individuals in the two groups are, on average, balanced on all dimensions that may be meaningful to the test. The only remaining difference between the groups is the new experience we are testing, ensuring our estimate of the impact of the new experience is not biased in any way.\n",
    "\n",
    "Do we really know that the new product experience is what caused the increase in engagement? What other explanations are possible? What if you also knew that the UI gave users a rebate on a product? Now we have more than one possible explanation for the increase in engagement: it could be the new product experience, it could be saving money. The key point is that we don’t know if the new product experience caused the increase in engagement.\n",
    "\n",
    "Here's an example where if we did ***not*** do an A/B test, we may be drawn to conclude that our new UI results in better user conversion (or engagement), whereas, it really does not. If we release a new vesion of a package and then we see an improvement\n",
    "in the engagement of users, we might be drawn to conclude cause and effect. But if we were more careful and we continued to sample both products as in the graph on the right, we can see that something else caused that jump in engagement, not the new release.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/a-b-testing-1.png\" width=900 />\n",
    "</center>\n",
    "\n",
    ">**Important**: Running A/B tests, where possible, allows us to substantiate *causality* and confidently make changes to the product knowing that our members have voted for them with their actions.\n",
    "\n",
    "There is a lot of statistics related to A/B tests, so this notebook is a good review before data science interviews.\n",
    "\n",
    "First, we import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.stats.api as sms\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Some plot styling preferences\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "font = {'family' : 'Helvetica',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 14}\n",
    "\n",
    "mpl.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testable hypothesis\n",
    "\n",
    "We next turn this idea into a **testable hypothesis**, a statement of the form `If we make change X, it will improve the member experience in a way that makes metric Y improve`. \n",
    "\n",
    "There are two types of mistakes we can make in acting on test results:\n",
    "\n",
    "A **false positive** (also called a **Type I error**) occurs when the data from the test indicates a meaningful difference between the control and treatment experiences, but in truth there is *no* difference (I call this an ***eager beaver*** error). \n",
    "\n",
    ">**Type I error**: This scenario is like having a medical test come back as positive for a disease when you are healthy. Oh no!\n",
    "\n",
    "The other error we can make in deciding on a test is a **false negative** (also called a **Type II error**), which occurs when the data do not indicate a meaningful difference between treatment and control, but in truth there *is* a difference (I call this a ***lazy sloth*** error). \n",
    "\n",
    ">**Type II error**: This scenario is like having a medical test come back negative — when you do indeed have the disease you are being tested for. Oh no!\n",
    "\n",
    "We need to make one of two decisions based on the data:\n",
    "- Sufficient evidence to conclude that the new UI affects member satisfaction, or \n",
    "- Insufficient evidence\n",
    "\n",
    "There are two possible truths, that we never get to know with complete certainty.\n",
    "\n",
    "For a given F1 season, there are two possible decisions (LH or MV), well, more really, but let's just say these two are the most talented drivers by far, and likewise there are two possible truths (winner or loser). This leads to a total of four possible outcomes, shown in the figure below. \n",
    "\n",
    "The same is true with A/B tests: we make one of two decisions based on the data (`sufficient evidence to conclude that the new UI affects member satisfaction` or `insufficient evidence`), and there are two possible truths, that we never get to know with complete uncertainty (`New UI truly affects member satisfaction` or `it does not`).\n",
    "\n",
    "If there are two possible outcomes and two possible actors for each outcome, that's a total of 4 possibilities, expressed below as true/false positive/negatives:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/a-b-testing-2.png\" width=900 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager beaver: False positives, p-value, and statistical significance\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/eager-beaver.png\" width=300 />\n",
    "</center>\n",
    "\n",
    "With a sound hypothesis and a clear understanding of the primary decision metric, we turn to the statistical aspects of designing an A/B test. \n",
    "\n",
    "This process generally starts by fixing the **acceptable false positive rate**. \n",
    "\n",
    "By convention, this false positive rate is usually set to 5%: for tests where there is not a meaningful difference between treatment and control, we’ll falsely conclude that there is a *statistically significant* difference 5% of the time. \n",
    "\n",
    "Tests that are conducted with this 5% false positive rate are said to be run at the 5% **significance level**.\n",
    "\n",
    "This false positive rate is closely associated with the statistical significance of the observed difference in metric values between the treatment and control groups, which we measure using the **p-value**. \n",
    "\n",
    "The p-value is the probability of seeing an outcome at least as extreme as our A/B test result, had there truly been no difference between the treatment and control experiences. \n",
    "\n",
    "The p-value tells us how sensitive we are to wide swings in statistical difference. If it's large, don't worry that much about differences in point estimates, they are probably due to chance. If it's small and you see differences in point estimates, it probably means the statistics *are* different and there is a very definite mechanism that is causing this (e.g. a pill that cures, more talent than before, etc.).\n",
    "\n",
    "The p-value is not the probability that the null hypothesis is true! The p-value is only the probability of observing results as extreme or more extreme than the observed data, given that the null hypothesis is true.\n",
    "\n",
    ">**Fact**: It is known that the p-value often overstates the evidence against the null hypothesis. Which is why pharamceuticals often use this statistic to justify to the FDA that their drug works and should be marketed. However, the opposite might also occur sometimes, as we saw in our IQ notebook.\n",
    "\n",
    "Here's an intuitive way to understand statistical significance and p-values, which have been confusing students of statistics for over a century.\n",
    "\n",
    "Say we want to know if LH is a better or worse driver than MV, in the sense that the probability of LH beating MV in a head-to-head race is *not* 0.5 (or 50%). \n",
    "\n",
    "This is called a [two-tailed test](https://en.wikipedia.org/wiki/One-_and_two-tailed_tests):\n",
    "\n",
    "To decide, let’s run the following experiment: we'll have LH and MV race each other 100 times and tally up wins. Because of randomness, weather conditions, tire pressures, or *noise*, we wouldn’t expect exactly 50 wins and 50 losses for LH. But how much of a deviation from 50 is *too much*? When do we have sufficient evidence to reject the baseline assertion that LH and MV are just as good? \n",
    "\n",
    "Would you be willing to conclude that LH is better if he wins 60 out of 100 races? 70? \n",
    "\n",
    "We need a way to align on a decision framework and understand the associated false positive rate.\n",
    "\n",
    "To build intuition, let’s run through a thought exercise.\n",
    "\n",
    "First, we’ll assume that LH and MV are just as good F1 drivers — this is our **null hypothesis**, which is always a statement of **status quo**, **equality**, or **nothing interesting happening**. \n",
    "\n",
    "We then seek compelling evidence against this null hypothesis from the data.\n",
    "\n",
    "### 1. Build null hypothesis hypothetical (look at black and blue bars only)\n",
    "To make a decision on what constitutes compelling evidence, we first calculate the ***theoretical probability*** of every possible outcome, assuming that the null hypothesis is true. \n",
    "\n",
    "For F1, that’s the probability of 100 races yielding zero wins for LH, one win, two wins, and so forth up to 100 wins — assuming that both drivers are just as talented. \n",
    "\n",
    "Skipping over the math, each of these possible outcomes and their associated probabilities are shown with the black and blue bars in the figure below (ignore the colors for now). Some of these bars are more probable than others. If the null hypothesis is in effect, 50% is the most likely amongst all possibilities.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/a-b-testing-3.png\" width=900 />\n",
    "</center>\n",
    "\n",
    "### 2. Compare with observation (solid red line)\n",
    "We can then compare this probability distribution of outcomes, calculated under the assumption that both drivers are as talented, to the data we’ve collected: Our ***observations***.\n",
    "\n",
    "Say we observe that 55% of 100 races are LH wins (the solid red line in the figure above). To quantify if this observation is compelling evidence that LH is more talented, we count up the probabilities associated with every outcome that is less likely than our observation (the red arrows). \n",
    "\n",
    "### 3. Sum probabilities outside (outside both red lines)\n",
    "Here, because we’ve made no assumptions about win or loss being more likely, we sum up the probabilities of 55% ***or more*** of the races ending up as wins for LH (the bars to the right of the solid red line) and the probabilities of 55% ***or more*** of the races coming up as losses for LH (the bars to the left of the dashed red line).\n",
    "\n",
    "And this is the mythical **p-value**! The probability of seeing a result ***as extreme as our observation***, if the null hypothesis were true. In our case, the null hypothesis is that both drivers are as talented, say the observation is 55% wins for LH in 100 races, and the p-value is about 0.32. \n",
    "\n",
    "The interpretation is as follows: Were we to repeat, many times (a.k.a. may samplings, or many parallel universes), the experiment of 100 LH/MV races and calculating the fraction of wins, with an equally talented predicate (the null hypothesis is true), then *in 32% of those experiments the outcome would feature at least 55% wins or at least 55% losses (results at least as unlikely as our actual observation)*.\n",
    "\n",
    "### 4. Compare with false positive accepted rate\n",
    "How do we use the p-value to decide if there is **statistically significant** evidence that one driver is more talented that the other? \n",
    "\n",
    "It comes back to that 5% false positive rate that we agreed to accept at the beginning: We conclude that there is a statistically significant effect if the p-value is less than 0.05. \n",
    "\n",
    "This formalizes the intuition that we should reject the null hypothesis that the drivers are qually as talented if our result is **sufficiently unlikely** to occur under the assumption of an equal talent level. \n",
    "\n",
    "In the example of observing 55 wins out of 100 head to head races, we calculated a p-value of 0.32. Because the p-value is larger than the 0.05 significance level, we conclude that there is *not statistically significant evidence that one driver is more talented than the other*.\n",
    "\n",
    "There are two conclusions that we can make from an experiment or A/B test: we either conclude there is an **effect** (*one driver is more talented than the other*, `the new UI increases member satisfaction`) or we conclude that there is *insufficient evidence to conclude there is an effect* (*cannot conclude that one driver is more talented*, `cannot conclude that the new UI increases member satisfaction`). \n",
    "\n",
    "The p-value is the Bernoulli-like decider on the effect. It is not a probability!\n",
    "\n",
    "In a race, where the two possible outcomes are *winner* or *loser* — *loser* is very different from *less talented* (I know, because I'm very talented in Tennis but I loose all the time! ;-)\n",
    "\n",
    "This (frequentist) approach to A/B testing when the p-value exceeds the threhsold does not allow us to make the conclusion that there is no effect — we never conclude one driver is better than the other, or that the new UI has no impact on our members. We just conclude we’ve *not collected enough evidence to reject the null assumption that there is no difference*. We observed 55% wins out of 100 races, and concluded we had insufficient evidence to label one driver as more talented. \n",
    "\n",
    "If we gathered more evidence, say with 1000 races, we might find sufficiently compelling evidence to reject the null hypothesis of an equal amount of talent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rejection Regions and Confidence Intervals\n",
    "There are two other concepts in A/B testing that are closely related to p-values: The **rejection region** for a test, and the **confidence interval** for an observation. \n",
    "\n",
    "### Rejection Regions\n",
    "Another way to build a decision rule for a test is in terms of what’s called a **rejection region** — the set of values for which we’d conclude that one driver is more talented. \n",
    "\n",
    "To calculate the rejection region, we once more assume the null hypothesis is true (equal amount of talent), and then define the rejection region as the set of least likely outcomes with probabilities that sum to no more than 0.05. \n",
    "\n",
    "The rejection region consists of the outcomes that are the most extreme — the outcomes where the evidence against the null hypothesis is strongest. \n",
    "\n",
    "If an observation falls in the rejection region, we conclude that there *is* statistically significant evidence that one of the two drivers is more talented, and *reject* the null hypothesis. \n",
    "\n",
    "In the case of our races, the rejection region corresponds to observing fewer than 40% or more than 60% wins for one driver (shown with blue shaded bars in the figure above). We call the boundaries of the rejection region, here 40% and 60% wins, the **critical values** of the test.\n",
    "\n",
    "There is an equivalence between the rejection region and the p-value, and both lead to the same decision: the p-value is less than 0.05 if and only if the observation lies in the rejection region.\n",
    "\n",
    "### Confidence Intervals\n",
    "So far, we’ve approached building a decision rule by first starting with the null hypothesis, which is always a statement of no change or equivalence (*equal talent* or `the new UI does not impact member satisfaction`). \n",
    "\n",
    "We then define possible outcomes under this null hypothesis and compare our observation to that distribution. \n",
    "\n",
    "To understand confidence intervals, it helps to flip the problem around to focus on the observation. We then go through a thought exercise: given the observation, what values would lead to a decision not to reject the null hypothesis of equal talent, assuming we specify a 5% false positive rate? \n",
    "\n",
    "The answer is an observation of 55% wins in 100 races and we do not reject the null hypothesis of equal talent. \n",
    "\n",
    "Nor would we reject the null hypothesis if the probability of wins was 47.5%, 50%, or 60%. \n",
    "\n",
    "There’s a whole range of values for which we would not reject the null, from about 45% to 65% probability of wins (Figure below).\n",
    "\n",
    "This range of values is a **confidence interval**: The set of values under the null hypothesis that would *not* result in a rejection of the null hypothesis, given the data from the test. \n",
    "\n",
    "Because we’ve mapped out the interval using tests at the 5% significance level, we’ve created a **95% confidence interval**. \n",
    "\n",
    "The interpretation is that, under repeated experiments, the confidence intervals will cover the true value (here, the actual probability of wins) 95% of the time.\n",
    "\n",
    "There is an equivalence between the confidence interval and the p-value, and both lead to the same decision: the 95% confidence interval does not cover the null value if and only if the p-value is less than 0.05, and in both cases we reject the null hypothesis of equal talent.\n",
    "\n",
    "Consider what happens as the null hypothesis (black curve below) coincides with observations (red line and red shaded area lying between rejection regions below):\n",
    "- If the peak of the null hypothesis is within the rejection region (outside the red shaded area), we ***reject*** the null hypothesis\n",
    "- If the peak of the null hypothesis is outside of the rejection region (inside the red shaded area), we ***accept*** the null hypothesis\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/a-b-testing-4.png\" width=900 />\n",
    "</center>\n",
    "\n",
    "So, we’ve built up intuition about false positives, statistical significance and p-values, rejection regions, confidence intervals, and the two decisions we can make based on test data. \n",
    "\n",
    "These core concepts and intuition map directly to comparing treatment and control experiences in an A/B test. We define a *null hypothesis* of no difference: the `B` experience does not alter affect member satisfaction. \n",
    "\n",
    "We then play the same thought experiment: what are the possible outcomes and their associated probabilities for the difference in metric values between the treatment and control groups, assuming there is no difference in member satisfaction? \n",
    "\n",
    "We can then compare the observation from the experiment to this distribution, just like with F1 races, calculate a p-value and make a conclusion about the test. And we can define rejection regions and calculate confidence intervals.\n",
    "\n",
    "But false positives are only one of the two mistakes we can make when acting on test results. The other type of mistake is a **false negative**, and the closely related concept is that of **statistical power**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy sloth: False negatives and statistical power\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/lazy-sloth.png\" width=350 />\n",
    "</center>\n",
    "\n",
    "A **false negative** occurs when the data do not indicate a meaningful difference between treatment and control, but in truth there is a difference. \n",
    "\n",
    "False negatives are closely related to the concept of **statistical power**, which gives the probability of a true positive given the experimental design and a true effect of a specific size. \n",
    "\n",
    "In fact, statistical power is simply one minus the false negative rate $\\beta$.\n",
    "\n",
    "Power involves thinking about possible outcomes given a specific assumption about the actual state of the world — similar to how we defined significance by first assuming the null hypothesis is true. \n",
    "\n",
    "To build intuition about power, let’s try to decide if one driver is more talented than the other using an experiment that calculates the fraction of wins out of 100 races. \n",
    "\n",
    "The distribution of outcomes under the null hypothesis that the amount of talent is equal is shown in black the figure below, peaking at 50%.\n",
    "\n",
    "Let’s work through what happens when we have a set of 100 races where, on average, 64% of the time, one driver wins. \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/a-b-testing-5.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "Because there is uncertainty or noise in our experiment, we don’t expect to see exactly 64 wins out of 100 races. But as with the null hypothesis that the talent is equaly distributed, we can calculate all possible outcomes if this specific **alternative hypothesis** is true. This distribution is shown with the red curve on the figure above, peaking at 64%.\n",
    "\n",
    "Visually, **power** is the difference between black (null hypothesis) and red (alternative hypothesis): The fraction of the alternative (red) distribution that lies *beyond* the critical values under the null hypothesis (the blue lines and black curve). \n",
    "\n",
    "Here, 80% of the alternative distribution (red) falls to the right of the taller blue line that demarcates the critical value of the upper rejection region. \n",
    "\n",
    "Assuming that the truth about the coin is that the probability of a win is 64%, then the power of this test is 80%. \n",
    "\n",
    "To be complete, there could also be a small part of the alternative (red) distribution that falls within the lower rejection region (to the left of the short blue line).\n",
    "\n",
    "The power of a test corresponds to a specific, postulated **effect size**. \n",
    "\n",
    "In our example, if in our test 64% of the time one driver wins, our test has 80% power to detect that one driver is more talented. \n",
    "\n",
    "The interpretation is as follows: if LH has a probability of winning of 64% compared to MV, and we repeatedly run the experiment of head to head race 100 times, and we make a decision at the 5% significance level, then we will correctly reject the null hypothesis that there is an equal amount of talent in about 4 out of every 5 experiments (80%). \n",
    "\n",
    "In other words, we are looking for an effect 14% above 50-50.\n",
    "\n",
    "And 20% of those repeated experiments will result in a false negative: we won't reject the null hypothesis that the talent is equally distributed, even though it really is.\n",
    "\n",
    "## Ways to increase power\n",
    "In designing an A/B test, we first fix the significance level (the convention is 5%: if there is no difference between treatment and control, we’ll see false positives 5% of the time), and then design the experiment to control false negatives.\n",
    "\n",
    "There are three primary levers we can pull to increase power and reduce the probability of false negatives:\n",
    "\n",
    "### 1. Increase Effect size\n",
    "Simply put, the larger the effect size — the difference in metric values between Groups A and B — the higher the probability that we’ll be able to correctly detect that difference. \n",
    "\n",
    "To build intuition, think about running an experiment to determine talent, where the data we collect is the fraction of wins in 100 races. \n",
    "\n",
    "Now think of two scenarios. In the first scenario, the true probability of a win 55%, and in the second it is 75%. \n",
    "\n",
    "Intuitively, it is more likely that our experiment identifies that one driver is more talented in the second scenario. \n",
    "\n",
    "The true probability of wins is further from the null value of 50%, so it’s more likely that an experiment will produce an outcome that falls in the rejection region. \n",
    "\n",
    "We could for example give both drivers *shitty cars* to drive ;-)\n",
    "\n",
    "In a better UI context, we can increase the expected magnitude of metric movements by being *bold* (switch to React from Angular) vs *incremental* (improve our Angular) with the hypotheses we test. \n",
    "\n",
    "Another strategy to increase effect sizes is to test in new areas of the product, where there may be room for larger improvements in member satisfaction. That said, one of the joys of learning through experimentation is the element of surprise: at times, seemingly small changes can have a major impact on top-line metrics.\n",
    "\n",
    "### 2. Increase Sample size\n",
    "The more races in the experiment, the higher the power and the easier it is to correctly identify smaller effects. \n",
    "\n",
    "To build intuition, think again about running an experiment to determine talent, where the data we collect is the fraction of wins in a fixed number of races and the true probability of a win is 64%. \n",
    "\n",
    "Consider two scenarios: in the first, we race 20 times, and in the second, we race 100 times. \n",
    "\n",
    "Intuitively, it is more likely that our experiment identifies one driver is more  talented in the second scenario. \n",
    "\n",
    "With more data, the result from the experiment is going to be closer to the true rate of 64% win probability, while the outcomes under the assumption of equal amount of talent concentrate around 0.50, causing the rejection region to encroach on the 50% value. \n",
    "\n",
    "These effects combine, so that with more data there is a greater probability that the result from the experiment where one driver is more talented will fall in that rejection region, resulting in a true positive. \n",
    "\n",
    "In a better UI context, we can increase the power by allocating more members to the test or by reducing the number of test groups.\n",
    "\n",
    "### 3. The variability of the metric in the underlying population\n",
    "The more homogenous the metric within the population we are testing on, the easier it is to correctly identify true effects. \n",
    "\n",
    "The intuition for this one is a bit trickier, and our UI examplar finally breaks down. Say that we run a test that aims to reduce some measure of latency, such as the delay between a member pressing play and video playback commencing. Given the variety of devices and internet connections that people use, there is a lot of natural variability in this metric across users. As a result, if the test treatment results in a small reduction in the latency metric, it’s hard to successfully identify what is going on — the *noise* from the variability across members overwhelms the small signal. \n",
    "\n",
    "In contrast, if we ran the test on a set of members that used *similar* devices with *similar* web connections, then the small signal is easier to identify — there is less noise that might drown out the signal. \n",
    "\n",
    "Thus, it is important to spend a lot of time building statistical analysis models that exploit this intuition, and increase power by effectively lowering variability.\n",
    "\n",
    "## Powering for reasonable and meaningful effects\n",
    "Power and the false negative rate are functions of a postulated effect size. \n",
    "\n",
    "Much like how the 5% false positive rate is a widely-accepted convention, the rule of thumb with power is to aim for 80% power for a reasonable and meaningful effect size. \n",
    "\n",
    "That is, we postulate an effect size and then design the experiment, primarily through setting the sample size, such that, if the true impact of the treatment experience is as we’ve postulated, the test will correctly identify that there is an effect 80% of the time. \n",
    "\n",
    "And 20% of the time the result from the test will be a false negative: in truth, there is an effect, but our observation from the test does not lie in the rejection region and we fail to conclude that there is an effect. \n",
    "\n",
    "That’s why the examples above used a 64% probability of wins: An experiment with 100 races then has 80% power.\n",
    "\n",
    "What constitutes a reasonable effect size can be tricky, as tests can surprise us. But a mix of domain knowledge and common sense can generally provide solid estimates. \n",
    "\n",
    "In an area where testing has a long history, companies have a solid idea about the effect sizes that their tests tend to produce (be they positive or negative). \n",
    "\n",
    "Given an understanding of past effect sizes, as well as the analysis strategy, companies can set the sample size to ensure the test has 80% power for a reasonable metric movement.\n",
    "\n",
    "The second consideration, both in this experimental design phase and in deciding where to invest efforts, is to determine what constitutes a meaningful impact to the primary metrics used to decide the test. \n",
    "\n",
    "What is meaningful will depend on the impact area of the experiment (member satisfaction, playback latency, technical performance of back end systems, etc.), and potentially the effort or costs associated with the new product experience. \n",
    "\n",
    "As a hypothetical, say that, for effect sizes smaller than a 0.1% change in the primary metric (what we'll see in our experiment at the end of this notebook), the cost of supporting the new product feature outweighs the benefits. In this case, there’s little point in powering a test to detect a 0.01% change in the metric, as successfully identifying an effect of that size won’t result in a meaningful change in decisions. Likewise, if the effect sizes seen in tests in a given innovation area are consistently immaterial to the user experience or the business, that’s a sign that experimentation resources can be more efficiently deployed somewhere else.\n",
    "\n",
    "This is called doing a [power analysis](https://en.wikipedia.org/wiki/Power_of_a_test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "An uncomfortable truth about experimentation is that we can’t simultaneously minimize both false positives and false negatives.\n",
    "\n",
    "In fact, false positives and negatives trade off with one another. \n",
    "\n",
    "If we used a more stringent false positive rate, such as 0.01%, we’d reduce the number of false positives for tests where there is no difference between A and B — but we’d also reduce the power of the test, increasing the rate of false negatives, for those tests where there is a meaningful difference. \n",
    "\n",
    ">**Empirical fact**: Using a 5% false positive rate and targeting 80% power are well-established conventions that balance between limiting false discovery and enabling true discovery. \n",
    "\n",
    "However, in instances where a false positive (or false negative) poses a larger risk, researchers may deviate from these rules of thumb to minimize one type of uncertainty over another!\n",
    "\n",
    "Our goal is not to eliminate uncertainty, but to understand and quantify the uncertainty in order to make sound decisions. \n",
    "\n",
    "In many cases, results from A/B tests require nuanced interpretation, and in fact the test result itself is only one input into a business decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing an experiment\n",
    "Let's go over the process of analysing an A/B experiment, from formulating a hypothesis, testing it, and finally interpreting results, from both a frequentist and a Bayesian perspective.\n",
    "\n",
    "We’ll use a dataset I downloaded from [kaggle](https://www.kaggle.com/ivanpl/a-b-testing/data) which contains the results of an A/B test on what seems to be 2 different designs of a website page (old UI vs. new UI). Let's hypothesize it's selling shampoos.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/shampoos.png\" width=300 />\n",
    "</center>\n",
    "\n",
    "Suppose the product manager (PM) tells you that the current **conversion rate** (how many people buy the shampoo over how many people view the Web page) is about 13% on average throughout the year, and that the team would be happy with an increase of 2%, meaning that the new UI design will be considered a success if it raises the conversion rate to 15%.\n",
    "\n",
    "Before rolling out the change, the team would be more comfortable testing it on a small number of users to see how it performs, so you suggest running an A/B test on a subset of your user base users.\n",
    "\n",
    "## 1. Formulating a hypothesis\n",
    "First things first, we want to make sure we formulate a hypothesis at the start of our project. This will make sure our interpretation of the results is correct as well as rigorous.\n",
    "\n",
    "Given we don’t know if the new design will perform better or worse (or the same?) as our current design, we’ll choose a [two-tailed test](https://en.wikipedia.org/wiki/One-_and_two-tailed_tests):\n",
    "\n",
    "$$Hₒ: p = pₒ$$\n",
    "\n",
    "$$Hₐ: p ≠ pₒ$$\n",
    "\n",
    "where $p$ and $pₒ$ stand for the conversion rate of the new and old design, respectively. We’ll also set a confidence level of 95%:\n",
    "\n",
    "$$α = 0.05$$\n",
    "\n",
    "The $α$ value is a threshold we set, by which we say `if the probability of observing a result as extreme or more (p-value) is lower than α, then we reject the Null hypothesis`. Since $α=0.05$ (indicating 5% probability), our confidence $(1 — α)$ is 95%.\n",
    "\n",
    "## 2. Choosing the variables\n",
    "For our test we’ll need two groups:\n",
    "- A control group - They'll be shown the *old* UI\n",
    "- A treatment (or experimental) group - They'll be shown the *new* UI\n",
    "\n",
    "This will be our **independent variable**. The reason we have two groups even though we know the baseline conversion rate is that we want to control for other variables that could have an effect on our results, such as **seasonality**. By having a control group we can directly compare their results to the treatment group, because the only systematic difference between the groups is the design of the product page, and we can therefore attribute any differences in results to the designs. See the first figure in this notebook to understand why having two groups is important.\n",
    "\n",
    "For our **dependent variable** (i.e. what we are trying to measure), we are interested in capturing the conversion rate. A way we can code this is by each user session with a binary variable:\n",
    "- 0 - The user did not buy the product during this user session\n",
    "- 1 - The user bought the product during this user session\n",
    "\n",
    "This way, we can easily calculate the mean for each group to get the conversion rate of each design.\n",
    "\n",
    "Let's do the first step of programming: *naming variables*!\n",
    "\n",
    "Suppose that we have obtained data from $n$ visitors, $n_A$ of which have been (randomly) sent to the old page $A$ and $n_B$ of which have been sent to new page $B$. \n",
    "\n",
    "Further, let $X_A$ and $X_B$ denote the number of visitors for whom we obtained a *successful* outcome (a conversion, a buy!) in the two groups. The proportion of successes in the two groups is then given by $p_A = \\frac{X_A}{n_A}$ and $p_B = \\frac{X_B}{n_B}$ respectively.\n",
    "\n",
    "The estimated difference in conversion rates is then give by the difference in proportions:  $p_A − p_B$.\n",
    "\n",
    "To assess whether we have statistical evidence that the two pages' conversion rates truly differ, we perform a **hypothesis test**:\n",
    "- The **null hypothesis** that we want to test for is that the two pages' conversion rates are equal\n",
    "- The **alternative** is that they differ (one is higher than the other). \n",
    "\n",
    "If $p_A = $ the proportion of the page $A$ population whom we obtained a successful outcome (conversion), and $p_B =$ the proportion of the page $B$ population whom we obtained a successful outcome (conversion, then we are interested in testing the following hypothesis:\n",
    "\n",
    "$H_0: p_A = p_B$ versus $H_1: p_A ≠ p_B$.\n",
    " \n",
    "Or put it in another way, the null hypothesis says that the factors `page version` and `outcome` are statistically *independent* of each other. In words, this means knowing which page someone is sent to tells you nothing about the chance that they will have a successful outcome. Now that we know what hypothesis test we're interested in, we'll have to derive the appropriate test statistic.\n",
    "\n",
    "## 3. Choosing a sample size\n",
    "It is important to note that since we won’t test the whole user base (our population), the conversion rates that we’ll get will inevitably be only estimates of the true rates.\n",
    "\n",
    "The number of people (or user sessions) we decide to capture in each group will have an effect on the precision of our estimated conversion rates: the larger the sample size, the more precise our estimates (i.e. the smaller our confidence intervals), the higher the chance to detect a difference in the two groups, if present.\n",
    "\n",
    "On the other hand, the larger our sample gets, the more expensive (and impractical) our study becomes.\n",
    "So how many people should we have in each group?\n",
    "\n",
    "The sample size we need is estimated through something called [Power analysis](https://en.wikipedia.org/wiki/Power_of_a_test), and it depends on a few factors:\n",
    "- Power of the test $(1 — β)$ — This represents the probability of finding a statistical difference between the groups in our test when a difference is actually present. This is usually set at 0.8 by convention\n",
    "\n",
    "- Alpha value ($α$) — The critical value we set earlier to 0.05\n",
    "\n",
    "- Effect size — How big of a difference we wish to observe between conversion rates\n",
    "\n",
    "Since our team would be happy with a difference of 2%, we can use 13% and 15% to calculate the effect size we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review \n",
    "**Statistical inference** is the process of analyzing **sample data** to gain insight into the population from which the data was collected and to investigate differences between data samples. \n",
    "\n",
    "**Point estimates** are estimates of population parameters based on sample data, resulting from some kind of aggregation operation. \n",
    "\n",
    "For instance, if we wanted to know the average age of students at NU, we could take a survey of students and then use the average age of the respondents as a point estimate of the average age of NU students as a whole. The average of a sample is known as the **sample mean**. \n",
    "\n",
    "The sample mean is usually not exactly the same as the population mean. This difference can be caused by many factors including the randomness inherent to drawing a sample from a population. \n",
    "\n",
    "In data analysis, we are often interested in the characteristics of some large population, but collecting data on the entire population may be infeasible. \n",
    "\n",
    "The [Central Limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) is one of the most important results of probability theory and serves as the foundation of many methods of statistical analysis. \n",
    "\n",
    ">**Central Limit theorem**: At a high level, the theorem states the distribution of many sample means, known as a sampling distribution, will be normally distributed! This rule holds even if the underlying distribution itself is not normally distributed! As a result we can treat the sample mean as if it were drawn normal distribution. \n",
    "\n",
    "That is why probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions. Here's an illustrative example (we already did this with the Celtics, but it's important enough and tends to be an interview question so I'll do it again): We concatenate two Poisson distributions to obtain a bimodal (two-humped) dataset and investigate point estimates by drawing a sample from it to estimate the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population mean: 43.002372\n"
     ]
    }
   ],
   "source": [
    "# generate some random number to serve as our population\n",
    "np.random.seed(10)\n",
    "population_ages1 = stats.poisson.rvs(loc = 18, mu = 35, size = 150000)\n",
    "population_ages2 = stats.poisson.rvs(loc = 18, mu = 10, size = 100000)\n",
    "population_ages = np.concatenate((population_ages1, population_ages2))\n",
    "print('population mean:', np.mean(population_ages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample 500 data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample mean: 42.388\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "sample_ages = np.random.choice(population_ages, size = 500)\n",
    "print('sample mean:', np.mean(sample_ages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real world data is often *not normally distributed* (I know that from all the parking tickets I get!) and the distribution of a sample tends to mirror the distribution of the population. This means a sample taken from a population with a skewed distribution will also tend to be skewed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['Helvetica'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['Helvetica'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(population_ages)\n",
    "plt.title('Population')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(sample_ages)\n",
    "plt.title('Sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots reveal that the data is clearly not normal: instead of one symmetric bell curve, it has as bimodal distribution with two high density peaks. Because of this, the sample we drew from this population should have roughly the same shape and skew.\n",
    "\n",
    "The sample indeed has roughly the same shape as the underlying population. Does this mean that we can't apply techniques that assume a normal distribution to this data set, since it is not normal?\n",
    "\n",
    "*No*!\n",
    "\n",
    "The central limit theorem states that the distribution of many sample means, known as a sampling distribution, will be normally distributed. This rule holds even if the underlying distribution itself is not normally distributed. As a result we can treat the sample mean as if it were drawn normal distribution. To illustrate, let's create a sampling distribution by taking 200 samples from our population and then making 200 point estimates of the mean.\n",
    "\n",
    "In other words, we repeat what we did above, but we repeat it 200 times with a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD5CAYAAAA3Os7hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVrUlEQVR4nO3df0zU9x3H8ZcHjC6lGT9GKRFFVxeCZaYnbdc0oaOTIKaEbaSuXaZLs5hVanKyWRcHdS6pbCTEbEGXzP7hBsW41RqQIROkDJRlK45mzCDaptpyyI+uIGSEHyLH/mj4BuTHHXLHHZ89H4nJl/t87/N5f75fvq/7evf9HqsmJycnBQBY8Wz+LgAA4B0EOgAYgkAHAEMQ6ABgCAIdAAxBoAOAIYL9NXBLS4u/hgaAFS05OXnOx/0W6NL8RU3X3t6uxMTEZajGd1b6HKjf/1b6HKjfexY6GeYtFwAwBIEOAIYg0AHAEAQ6ABiCQAcAQxDoAGAIAh0ADEGgA4Ah/HpjEbCQdQfOebG3G4ta++PC5704NrA8OEMHAEMQ6ABgCAIdAAxBoAOAIQh0ADDEogPd4XAoISFBCQkJcjgc1uPj4+M6duyYtmzZoqSkJD377LMqKCjQ0NCQVwsGAMxtUZctnjlzRjU1NXO25eXlqbKyUjabTfHx8ers7FRpaamuXbumkpIS2Wz8ZwAAfMnjlO3o6NDhw4dlt9v1yCOPzGhra2tTZWWlJCk/P1/nz59XcXGxJKm5uVl1dXVeLBkAMBePAv3u3bt67bXXZLPZVFRUpKCgoBntFy9etJbT09MlSampqQoNDZUkNTU1eateAMA8PHrL5dixY2ptbVVRUZHWrFkzq727u9tajoqKkiTZbDZFRESop6dHXV1dc/bb3t7uduzR0VGP1gtkK30OK73++xFo813p+4D6l4fbQL9y5YrefPNNZWVlKSsra1GdT05OLtjuyd/oC6S/5Xe/Vvoc/Ff/4m7X96ZA21/8DvlXINW/pL8p+uGHH2piYkI1NTWy2+2y2+3WGXddXZ3sdrsefvhha/2+vj5Jksvl0sDAgCQpNjZ2KfUDADzg8YeiY2NjGh4e1vDwsHXmPTExoeHhYaWmplrr1dbWSpIaGho0NjYmSUpJSfFiyQCAubgN9OzsbF2/fn3Gv9WrV0uStm7dquvXryspKUmZmZmSpIKCAm3bts26Rj05OVlpaWk+nAIAQPLi1+cWFhYqPj5eFRUVcjqdCg8PV0ZGhnJzc7kGHQCWwX0Fen19/azHQkJC5HA4Ztw9CgBYPpw6A4AhCHQAMASBDgCGINABwBAEOgAYgkAHAEMQ6ABgCAIdAAxBoAOAIQh0ADAEgQ4AhiDQAcAQBDoAGIJABwBDEOgAYAgCHQAMQaADgCEIdAAwhNf+pihgknUHzvll3I8Ln/fLuDADZ+gAYAgCHQAMQaADgCEIdAAwBIEOAIYg0AHAEAQ6ABiCQAcAQxDoAGAIAh0ADEGgA4AhCHQAMASBDgCGINABwBB8fS7c2lZyQ9INf5fxf2Hhr+313T7ga3vNwBk6ABiCQAcAQxDoAGAIAh0ADOHRh6KnT5/WqVOn1NnZqeHhYYWHh2vjxo360Y9+pCeeeEKSND4+ruPHj6u8vFy9vb2KjIzU1q1btXfvXoWFhfl0EgAAD8/Q33//ffX09Cg2Nlbr16/X7du31djYqB/+8Ifq7OyUJOXl5eno0aPq6upSXFyc+vv7VVpaqpycHLlcLp9OAgDg4Rn6L37xC4WGhlo/nz59Wq+//rrGxsbU1tamwcFBVVZWSpLy8/O1Y8cO1dfXKycnR83Nzaqrq1N6erpvZgAAkORhoIeGhury5csqKirSyMiIbt68aT2elJRkhbkkK7hTU1MVGhqqsbExNTU1EegA4GMe31g0ODio1tZW6+eoqCgVFxdr9erV6u7unvG4JNlsNkVERKinp0ddXV1z9tne3u523NHRUY/WC2QmzAFm8/Xv50o/BlZK/R4Helpamq5du6bPPvtMv/vd71RWVqZ9+/bp1KlT8z5ncnJywT4TExPdjtve3u7ReoFs5c+Bu0RN5+vfz5V+DARS/S0tLfO2LeqyxVWrVik6Olo//vGPJUk9PT364x//qNjYWGudvr4+SZLL5dLAwIAkzWgHAPiG20AfGRnR22+/rdHRUeux+vp6a3l4eFgpKSnWz7W1tZKkhoYGjY2NSdKMdgCAb7h9y2V8fFwHDx7UG2+8obVr1+rOnTvq6OiQJIWEhCgrK0tJSUnKzMxUVVWVCgoKdPLkSTmdTklScnKy0tLSfDsLAID7QA8NDVVWVpZaW1t169YtjY+PKzo6Wna7Xbt27dKmTZskSYWFhYqPj1dFRYWcTqfCw8OVkZGh3Nxc2WzckAoAvuZRoBcVFbntKCQkRA6HQw6HwyuFAQAWh1NnADAEgQ4AhiDQAcAQBDoAGIJABwBDEOgAYAgCHQAMQaADgCEIdAAwBIEOAIYg0AHAEAQ6ABiCQAcAQxDoAGAIAh0ADEGgA4AhCHQAMASBDgCGINABwBAEOgAYgkAHAEMQ6ABgCAIdAAxBoAOAIQh0ADAEgQ4AhiDQAcAQBDoAGIJABwBDEOgAYAgCHQAMQaADgCEIdAAwRLC/C4Bn1h045+8SAAQ4ztABwBAEOgAYgkAHAEMQ6ABgCAIdAAzh9iqXEydOqKGhQTdv3tTt27cVGRmpxx9/XHv27FFCQoIkaXx8XMePH1d5ebl6e3sVGRmprVu3au/evQoLC/P5JAAAHpyhl5WV6b333pPNZtPq1avV29urmpoavfTSS+rs7JQk5eXl6ejRo+rq6lJcXJz6+/tVWlqqnJwcuVwun08CAOBBoL/wwgt699131djYqJqaGh04cECSNDw8rLq6OrW1tamyslKSlJ+fr/Pnz6u4uFiS1NzcrLq6Oh+WDwCY4jbQX331VcXFxVk/P/nkk9ZySEiILl68aP2cnp4uSUpNTVVoaKgkqampyWvFAgDmt+g7RcvKyiRJ4eHh2rZtm37zm99YbVFRUZIkm82miIgI9fT0qKura96+2tvb3Y43Ojrq0XqBzIQ5wGy+/v1c6cfASqnf40C/c+eOXn/9dZ09e1ZhYWH67W9/q8jIyHnXn5ycdNtnYmKi23Xa29s9Wi+QeWcON7xSCzAXXx9jK/04DqT6W1pa5m3z6LLF/v5+vfzyyzp79qyio6P11ltv6YknnpAkxcbGWuv19fVJklwulwYGBma1AwB8x22gf/TRR/rud7+rlpYWJSYm6p133tHGjRut9pSUFGu5trZWktTQ0KCxsbFZ7QAA33H7lsuePXvkdDolSRMTE3I4HFbb9u3btX37dmVmZqqqqkoFBQU6efKktX5ycrLS0tJ8VDoAYDq3gX7nzh1r+YMPPpjRNnX2XVhYqPj4eFVUVMjpdCo8PFwZGRnKzc2VzcbNqACwHNwGen19vdtOQkJC5HA4Zpy9AwCWF6fPAGAIAh0ADEGgA4AhCHQAMASBDgCGINABwBAEOgAYYtHftgjAPOsOnFuGUeb+grmPC59fhrH/P3CGDgCGINABwBAEOgAYgkAHAEMQ6ABgCAIdAAxBoAOAIQh0ADAEgQ4AhiDQAcAQBDoAGIJABwBDEOgAYAgCHQAMQaADgCEIdAAwBIEOAIYg0AHAEAQ6ABiCQAcAQxDoAGAIAh0ADEGgA4AhCHQAMASBDgCGINABwBAEOgAYgkAHAEMQ6ABgCAIdAAzhUaBfvnxZr7zyip555hklJCQoISFBR48enbHO+Pi4jh07pi1btigpKUnPPvusCgoKNDQ05JPCAQAzeRTobW1tunTpkr70pS/Nu05eXp6OHj2qrq4uxcXFqb+/X6WlpcrJyZHL5fJawQCAuXkU6N/61rfU0tKid955Z872trY2VVZWSpLy8/N1/vx5FRcXS5Kam5tVV1fnpXIBAPPxKNAjIiL0xS9+cd72ixcvWsvp6emSpNTUVIWGhkqSmpqallIjAMADwd7opLu721qOioqSJNlsNkVERKinp0ddXV1zPq+9vd1t36Ojox6tt1y2ldy4z2fe7/MAswXS8T2fQMuh+Xgl0OczOTm5YHtiYqLbPtrb2z1ab/kQzIA3BdbxPbdAyqGWlpZ527xy2WJsbKy13NfXJ0lyuVwaGBiY1Q4A8A2vBHpKSoq1XFtbK0lqaGjQ2NjYrHYAgG949JZLbW2tioqKZryF8tZbb6myslKbNm3SkSNHlJmZqaqqKhUUFOjkyZNyOp2SpOTkZKWlpfmmegCAxaNAHxoaUkdHx4zHBgcHNTg4qEceeUSSVFhYqPj4eFVUVMjpdCo8PFwZGRnKzc2VzcYNqQDgax4FenZ2trKzsxdcJyQkRA6HQw6HwyuFAQAWh1NnADAEgQ4AhiDQAcAQBDoAGIJABwBDEOgAYAgCHQAMQaADgCF8+m2LAODOugPn/DLux4XP+2VcX+IMHQAMQaADgCEIdAAwBIEOAIYg0AHAEAQ6ABiCQAcAQxDoAGAIAh0ADEGgA4AhCHQAMASBDgCGINABwBAEOgAYgkAHAEMQ6ABgCAIdAAxBoAOAIQh0ADAEgQ4AhiDQAcAQBDoAGIJABwBDBPu7AADwh3UHzi3yGTe8NvbHhc97ra/pOEMHAEMQ6ABgCAIdAAxBoAOAIVbkh6KL/zADAMzn9TP0c+fO6Tvf+Y42bdqkp556Sg6HQ5988om3hwEA3MOrgX769Gn95Cc/0dWrVxUdHa2JiQnV1NToe9/7nv7zn/94cygAwD28Fuh37tzRkSNHJElbt27Vu+++q+rqaj344IPq6+vT8ePHvTUUAGAOXgv0K1eu6Pbt25Kk9PR0SVJMTIwef/xxSdKlS5e8NRQAYA5e+1C0p6fHWo6Kipq13N3dPes5LS0tHvV973pntj9yPyUCQEDwNPsWy2uBPjk5uajHk5OTvTU0AEBefMslNjbWWu7r67OW+/v7Z7UDALzPa4H+ta99TeHh4ZKk2tpaSVJvb6/+9a9/SZJSUlK8NRQAYA6rJud7T+Q+/OlPf9LPf/5zSVJcXJwGBgY0NDSkiIgInT17VjExMd4aCgBwD68GuiRVVlbqxIkT+uijjxQaGqqnn35a+/bt0/r16yVJDodDNTU1kj6/vLG4uFjS5y8Gf/7zn9XW1qbh4WFJUmlpqb7+9a+7HXPnzp1qbm6e9fjmzZt16tQpb01N0tz1Dw0Nqbi4WP/85z/V1dWl4eFhxcTE6LnnntPu3bsVGRm5YJ/j4+M6fvy4ysvL1dvbq8jISG3dulV79+5VWFhYwNe/nNt/vjlI0uHDh9XU1KTe3l5NTEzoy1/+sp5++mnt2bNHq1evXrBPf++Dpdbv72NguqGhIX3729+W0+mUJB08eFA7duxYsM/l3P6+msNyHwdz8fqt/1lZWcrKypqz7cyZM9ZGvFdjY6OuXLmi6OhoK9AXa82aNTPC56tf/ep99TOf+eofGBhQSUmJgoKCtHbtWgUHB6ujo0MlJSX6xz/+oYqKCtls87+7lZeXp8rKStlsNsXHx6uzs1OlpaW6du2aSkpKFnxuINQ/xdfbX1r4d+ivf/2rXC6X1q9fr6GhIX3yySc6c+aM3n//fZ0/f37Bfv29D5Za/xR/HQPTvfHGG1YQemq5tr/kuzlMWY7jYD7L9l0uHR0dOnz4sOx2u7q7u2dc5ihJhw4dUlRUlC5duqTdu3ff1xivvvqqsrOzvVHuLAvVHxoaqv379+vFF1/UQw89pLt37yo3N1cXLlzQ9evXde3aNW3cuHHOftva2lRZWSlJys/P144dO1RfX6+cnBw1Nzerrq7Ouq4/EOufzpfb390cJKm6ulqhoaHWz/v371dlZaVu3ryp27dvKyIiYs5+A2EfLKX+6fx1DEyprq5WRUWFtm3bpr/85S8e9btc29+Xc5jO18fBQpbl2xbv3r2r1157TTabTUVFRQoKCpq1TkxMjIKDl/b68qtf/UpJSUnasmWLDh48qM8++2xJ/U1xV390dLR27dqlhx56SJIUHByszZs3W+1f+MIX5u374sWL1vLUL21qaqp1YDc1NQV0/dP5avt7Mgfp8xemP/zhD9q+fbvS09OtkNiwYYP1gf1cAmEfLKX+6fx1DEif32ty6NAhPfbYY8rNzfW47+XY/pJv5zCdL48Dd5Yl0I8dO6bW1lYdOnRIa9as8ckYDzzwgGJiYhQZGanOzk69/fbbevHFF+/77ZvpFlv/0NCQysvLJUlPPvmkNmzYMO+602+4mroJy2azWWdjXV1dSyldkm/rn+LL7S95Podbt27p3//+t/WFcI899phOnDihVatWzfucQNoH91P/FH8eAy6XSz/96U919+5dHTlyZFEnZ8ux/SXfzmGKr48Dd3we6FeuXNGbb7654HvrS/Wzn/1Mly9fVlVVlRobG/XKK69Ikjo7O3XhwoUl9b3Y+nt7e7Vz50598MEH2rBhg37961/f17je+qx6Oer35fZf7Bzy8/N19epVVVdX66mnnlJbW5v279+viYmJRY/rj31wv/X7+xgoLS1Vc3Oz8vLyrAsglsqb12ssxxx8fRx4wueB/uGHH1rfumi322W3261X3Lq6Otntdv33v/9d0hgbN2603hZYtWqVMjMzrba5vnJgMRZTf1tbm1544QVdvXpVmzdvVllZmaKjoxfsf64bslwulwYGBma1B2L9km+3/2LnIElBQUF69NFH9fLLL0uS3nvvPf3973+ft/9A2gf3U7/k/2Pg8uXLkqRf/vKXstvtM8YvLCzUSy+9NG//vt7+yzEHyffHgSeW7S8WjY2NaXh4WMPDw9Yr78TExIyf3ent7VVGRoYyMjKsV7y+vj79/ve/19DQkLVedXW1tezuci9v1X/hwgV9//vf16effqrMzEyVlJTM+hBrrvqn33A1dUNWQ0ODxsbGZrUHYv3Ltf3dzeH69etqbGy0Hne5XGpsbLSeOzIyMu8cAmEfLKX+QDkGJFntU/VKn1+SGAjb35dzWM7jYCFevw7dE9/85jd169atGdd/FhUVqba2ViMjI9Z3pz/88MN64IEHtHPnTv3gBz9QZ2entmzZIunzDx6ys7Otx4KDg7V27VqNjIxYr4aPPvqoysvLZ1w54Iv6e3t79Y1vfEOTk5MKCgpSUlLSjPWnPmSZq35J2rdvn6qqqmSz2bRu3To5nU6Nj48rOTlZZWVlXr1ky9v1+2P7zzWHuro67dmzRw8++KDi4uLU19dnfRgVGxurqqoqhYWFBew+WEr9gXAM3Gt6ndOv4Q6U7e/tOfjrOLhXwPwJur6+PnV0dMx47NNPP5UkDQ4Ozvu8yMhI7d69W3/729/kdDo1Ojqqr3zlK0pLS9OuXbuWZSOOj4/PeLVvbW2d0T79VXsuhYWFio+PV0VFhZxOp8LDw5WRkaHc3Fyf/CLfayn1B8L2l6T4+Hg999xzunr1qm7cuGE99swzzygnJ8ftzSn+3gdLqT9Q9sFS+Hv7L1Wg7AO/nKEDALwv8F/6AAAeIdABwBAEOgAYgkAHAEMQ6ABgCAIdAAxBoAOAIQh0ADAEgQ4Ahvgf/1Z/nk0C76IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "samples = 200\n",
    "point_estimates = [np.random.choice(population_ages, size = 500).mean()\n",
    "                   for _ in range(samples)]\n",
    "\n",
    "plt.hist(point_estimates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, well how about that?! The sampling distribution appears to be roughly normal, despite the bimodal population distribution that the samples were drawn from. \n",
    "\n",
    "In addition, the mean of the sampling distribution approaches the true population mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_ages.mean() - np.mean(point_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we collect *a large number* of different sample means from the population, the sampling distribution, the distribution of the sample means collected, will approximately take the shape of a normal distribution around the population mean *no matter what the orginal population distribution is*.\n",
    "\n",
    "Of course, that implies a lot of sampling, but this procedure is essential in order to be able to apply classical frequentist results that most of the time depend on gaussian (or student-T) profiles.\n",
    "\n",
    ">**Note**: In machine learning, the equivalent concept is [k-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) resampling.\n",
    "\n",
    ">**Note**: p-values are based on calculating the probability of observing test statistics that are as extreme or more extreme than the test statistic actually observed, which relies on hypothetical repeated samples, which we do not observe. Sometimes, they're described derisively **parallel universes** by Bayesians that prefer relying on observed data rather than repeated sampling (one per universe).\n",
    "\n",
    "So, for a concrete example, I would like to give you homework, my dear class, to tell me what are *my chances of marrying Dua Lipa*. \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/dua-lipa-marry-professor.jpg\" width=500 />\n",
    "</center>\n",
    "\n",
    "So you say, ok, there are many professors that want to marry Dua Lipa out in the world (and that have very small pickings to that claim). How would you design an experiment for this? Let's go sampling!\n",
    "\n",
    "I'm going to go sample in India, then in China, then in Greece, then in the US, then across all rich countries of the world, then across all poor countries, then across all cold countries, then across all warm-weather countries, etc. That way, I'm going to generate *many* samplings. For each sampling, I'm going to perform an aggregation operation: An operation yielding a point estimate, such as mean or a higher order moment, telling me something about professors marrying Dua Lipas. Then I'm going to put all these point estimates together to generate a pdf that, or so the Central Limit Theorem tells me, should look like a normal distribution. \n",
    "\n",
    "Then I'm going to build myself a null hypothesis: ***Can't tell a difference between professors or rock stars marrying other rock stars in the general population***, and an alternative hypothesis: ***Professors do not marry Dua Lipas, are you crazy?!***  And then I'm going to use a z-test (normal likelihoods), or a t-test (student-T likelihoods), or ..., to generate a p-value that will tell me the probability of observing the results I observe if the null hypothesis is in effect. Based on the p-value and the observations, I'll be able to tell professor if he should propose to Dua Lipa, or whether he should probably not waste his time or ego. And then, you get an A for the class :-)\n",
    "\n",
    ">**Bayesian science**: A Bayesian scientist would also sample to generate observations, but he would proceed differently. Instead of generating point estimates from different samplings, she would look at the histogram of all the observations (samples), decide on a likelihood function, figure out pdfs for the parameters, build two different models (marries Dua Lipa, does not marry Dua Lipa), perform mcmc simulations to match the model with the observations, and then use the Bayes factor to figure out which model is more likely to match observations (more on this later). And you also get an A :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **point estimate** can give us a rough idea of a population parameter like the mean, but estimates are prone to error. A **confidence interval** is a range of values above and below a point estimate that captures the true population parameter at some predetermined confidence level. For example, if you want to have a 95% chance of capturing the true population parameter with a point estimate and a corresponding confidence interval, we'd set our confidence level to 95%.\n",
    "\n",
    ">**Note**: Confidence intervals is how frequentist statisticians try to get a hold of the standard deviation (error) of the point estimate. In truth, it's a much poorer alternative to Bayesian standard deviations!\n",
    "\n",
    "This is how frequentists determine confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "red = sns.xkcd_rgb['vermillion']\n",
    "blue = sns.xkcd_rgb['dark sky blue']\n",
    "\n",
    "sample_size = 1000\n",
    "sample = np.random.choice(population_ages, size = sample_size)\n",
    "sample_mean = sample.mean()\n",
    "\n",
    "confidence = 0.95\n",
    "z_critical = stats.norm.ppf(q = confidence + (1 - confidence) / 2)\n",
    "print('z-critical value:', z_critical)                     \n",
    "\n",
    "pop_stdev = population_ages.std()\n",
    "margin_of_error = z_critical * (pop_stdev / np.sqrt(sample_size))\n",
    "confint = sample_mean - margin_of_error, sample_mean + margin_of_error\n",
    "print('point esimate:', sample_mean)\n",
    "print('Confidence interval:', confint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the confidence interval we calculated captures the true population mean of 43.0023. \n",
    "\n",
    "Let's create several confidence intervals and plot them to get a better sense of what it means to \"*capture*\" the true mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "confidence = 0.95\n",
    "sample_size = 1000\n",
    "\n",
    "intervals = []\n",
    "sample_means = []\n",
    "for sample in range(25):\n",
    "    sample = np.random.choice(population_ages, size = sample_size)\n",
    "    sample_mean = sample.mean()\n",
    "    sample_means.append(sample_mean)\n",
    "\n",
    "    z_critical = stats.norm.ppf(q = confidence + (1 - confidence) / 2)                    \n",
    "    pop_std = population_ages.std()\n",
    "    margin_error = z_critical * (pop_stdev / np.sqrt(sample_size))\n",
    "    confint = sample_mean - margin_error, sample_mean + margin_error \n",
    "    intervals.append(confint)\n",
    "    \n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.errorbar(x = np.arange(0.1, 25, 1), y = sample_means, \n",
    "             yerr = [(top - bot) / 2 for top, bot in intervals], fmt = 'o')\n",
    "\n",
    "plt.hlines(xmin = 0, xmax = 25,\n",
    "           y = population_ages.mean(), \n",
    "           linewidth = 2.0, color = red)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the plot above, all but one of the 95% confidence intervals (error bars) overlap the red line marking the true mean. This is to be expected: Since a 95% confidence interval captures the true mean 95% of the time, we'd expect our interval to miss the true mean 5% of the time.\n",
    "\n",
    "More formally, the definition of a 95% confidence interval means that 95% of confidence intervals, created based on random samples of the same size from the same population will contain the true population parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of review, back to our experiment\n",
    "\n",
    "Ok, we already did A/B tests with IQ medicine (works/doesn't), but let's do it again. An A/B test involves a **null hypothesis** and an **alternative hypothesis**.\n",
    "\n",
    "We conduct a hypothesis test under the assumption that the null hypothesis is true. If the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, we stick with the null hypothesis. If they do, then we reject the null hypothesis in favor of the alternative.\n",
    "\n",
    "Frequentist statistic's hypothesis testing uses a **p-value** to weigh the strength of the evidence (what the data is telling us). p-value is defined as the probability of obtaining the observed or more extreme outcome, given that the null hypothesis is true (*not the probability that the alternative hypthesis is true*). It is a number between 0 and 1 and interpreted in the following way:\n",
    "\n",
    "- A small p-value (typically <= 0.05, 0.05 is a commonly used threshold, the threshold is often denoted as  α) indicates **strong evidence** against the null hypothesis, so we reject the null hypothesis. This means that something interesting is going on and it’s not just noise!\n",
    "- A large p-value (> 0.05) indicates **weak evidence** against the null hypothesis, so we fail to reject the null hypothesis. - -Although p-value is still in our favor, we cannot conclusively say that it was not due to random noise.\n",
    "- p-values very close to the cutoff (0.05) are considered to be **marginal** (could go either way, also called *anecdotal*, even though this terminology is usualy reserved to Bayesian approaches). If you carefully read good papers on these kind of topics, you will always see the p-values being *reported* so that the readers can draw their own conclusions. I so wish politicians would do the same thing about their assertions!!\n",
    "\n",
    "We start with a *frequentist* approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical frequentist statistics\n",
    "In statistics, an [effect size](https://en.wikipedia.org/wiki/Effect_size) is a number measuring the strength of the relationship between two variables in a population, or a sample-based estimate of that quantity. \n",
    "\n",
    "Examples of effect sizes include the correlation between two variables, the mean difference, or the risk of a particular event happening. \n",
    "\n",
    "The following four quantities have an intimate mathematical relationship:\n",
    "- Sample size\n",
    "- Effect size\n",
    "- Significance level = P(Type I error) = probability of finding an effect that is *not* there\n",
    "- Power = 1 - P(Type II error) = probability of finding an effect that is there\n",
    "\n",
    "We know that the current conversion rate is about 13% on average throughout the year and that the PM will be happy with 15%. So the effect size can be determined from these numbers. We need a *standard* way to convert it to a universal math factor.\n",
    "\n",
    "I went looking for the source code behind [statmodels power analysis](https://www.statmethods.net/stats/power.html), and found it [here](https://www.statsmodels.org/dev/_modules/statsmodels/stats/proportion.html#proportion_effectsize). It looks like it can be computed thusly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0576728617308947"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (np.arcsin(np.sqrt(.13)) - np.arcsin(np.sqrt(.15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arc sin (inverse sine) and the arc tan (inverse tangent) have the opposite effect: Arc sin squashes differences while the arc tan exaggerates differences (you'll see that when you study neural models). But that's about my only insight as to the formula.\n",
    "\n",
    "Let's see what `statmodels` says:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0576728617308947"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.proportion_effectsize(0.13, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup!\n",
    "\n",
    "So we set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size = sms.proportion_effectsize(0.13, 0.15)    # Calculating effect size based on our expected rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what's the right **sample size** based on our **significance level** (alpha = 5%), **effect size** (above), and **power** (80%)?\n",
    "\n",
    "We have already specified the three key components of a power analysis:\n",
    "\n",
    "- A decision rule of when to reject the null hypothesis. We reject the null when the p-value is less than 5%\n",
    "- Our tolerance for committing type 2 error (1 − 80% = 20%)\n",
    "- The detectable difference, i.e. the level of impact we want to be able to detect with our test\n",
    "\n",
    "Recall, statistical **power** is the probability of rejecting the null hypothesis when it is false. \n",
    "\n",
    "<center>\n",
    "<img src=\"ipynb.images/superman_logo.jpeg\" width=300 />\n",
    "</center>\n",
    "\n",
    "Hence for us to calculate the power, we need to define what *false* means to us in the context of the study. In other words, how much impact, i.e., *difference between test and control*, do we need to observe in order to reject the null hypothesis and conclude that the action worked?\n",
    "\n",
    "If we think that an event rate reduction of, say,  $10^{-10}$%  is enough to reject the null hypothesis, then we need a ***huge*** sample size to get a power of 80%!\n",
    "\n",
    "That is because if the difference in event rates between the experimental group and the control group is such a small number, the null and alternative probability distributions will be nearly indistinguishable. Hence we will need to increase the sample size in order to move the alternative distribution to the right and gain power. \n",
    "\n",
    "Conversely, if we only require a reduction of 2% in order to claim success, we can make do with a much smaller sample size.\n",
    "\n",
    ">**Rule**: The smaller the detectable difference, the larger the required sample size\n",
    "\n",
    "Effect size, power, and significance level are intimately related: [statmodels](https://www.statsmodels.org/dev/generated/statsmodels.stats.power.NormalIndPower.html(statmodels)) says that the required sample size as given by a **z-test** which assumes normal distributions is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4720"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_n = sms.NormalIndPower().solve_power(\n",
    "    effect_size, \n",
    "    power=0.8, \n",
    "    alpha=0.05, \n",
    "    ratio=1\n",
    "    )                                                  # Calculating sample size needed\n",
    "\n",
    "required_n = ceil(required_n)                          # Rounding up to next whole number                          \n",
    "required_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means we need at least 4720 observations for each group.\n",
    "\n",
    "Having set the power parameter to 0.8 means that if there exists an actual difference in conversion rate between our designs, assuming the difference is the one we estimated (13% vs. 15%), we have about 80% chance to detect it as statistically significant in our test with the calculated sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collecting and preparing the data\n",
    "So now that we have our required sample size, we need to collect the data. \n",
    "\n",
    "Usually at this point you would work with your team to set up the experiment, likely with the help of the Engineering team, and make sure that you collect enough data based on the sample size needed.\n",
    "\n",
    "We’ll download the dataset, read the data into a pandas DataFrame, do EDA by checking and cleaning the data as needed, then randomly sample $n=4720$ rows from the DataFrame for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.converted.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure all the control groupg are seeing the old page and viceversa\n",
    "pd.crosstab(df['group'], df['landing_page'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 294478 rows in the DataFrame, each representing a user session, as well as 5 columns :\n",
    "- user_id - The user ID of each session\n",
    "- timestamp - Timestamp for the session\n",
    "- group - Which group the user was assigned to for that session {control, treatment}\n",
    "- landing_page - Which design each user saw on that session {old_page, new_page}\n",
    "- converted - Whether the session ended in a conversion or not (binary, 0=not converted, 1=converted)\n",
    "\n",
    "We’ll actually only use the group and converted columns for the analysis.\n",
    "\n",
    "Before we go ahead and sample the data to get our subset, let’s make sure there are no users that have been sampled multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_counts = df['user_id'].value_counts(ascending=False)\n",
    "multi_users = session_counts[session_counts > 1].count()\n",
    "\n",
    "print(f'There are {multi_users} users that appear multiple times in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are, in fact, 3894 users that appear more than once. Since the number is pretty low, we’ll go ahead and remove them from the DataFrame to avoid sampling the same users twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_drop = session_counts[session_counts > 1].index\n",
    "\n",
    "df = df[~df['user_id'].isin(users_to_drop)]\n",
    "print(f'The updated dataset now has {df.shape[0]} entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our DataFrame is nice and clean, we can proceed and sample $n=4720$ entries for each of the groups. \n",
    "\n",
    "We can use pandas' `DataFrame.sample()` method to do this, which will perform Simple Random Sampling for us.\n",
    "\n",
    "We set random_state=22 so that the results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_sample = df[df['group'] == 'control'].sample(n=required_n, random_state=22)\n",
    "treatment_sample = df[df['group'] == 'treatment'].sample(n=required_n, random_state=22)\n",
    "\n",
    "ab_test = pd.concat([control_sample, treatment_sample], axis=0)\n",
    "ab_test.reset_index(drop=True, inplace=True)\n",
    "ab_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_test['group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualising the results\n",
    "The first thing we can do is to calculate some basic statistics to get an idea of what our samples look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_rates = ab_test.groupby('group')['converted']\n",
    "\n",
    "std_p = lambda x: np.std(x, ddof=0)              # Std. deviation of the proportion\n",
    "se_p = lambda x: stats.sem(x, ddof=0)            # Std. error of the proportion (std / sqrt(n))\n",
    "\n",
    "conversion_rates = conversion_rates.agg([np.mean, std_p, se_p])\n",
    "conversion_rates.columns = ['conversion_rate', 'std_deviation', 'std_error']\n",
    "\n",
    "\n",
    "conversion_rates.style.format('{:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by the stats above, it does look like our two designs performed very similarly, with our new design performing slightly better, approx. 12.3% vs. 12.6% conversion rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "sns.barplot(x=ab_test['group'], y=ab_test['converted'], ci=False)\n",
    "\n",
    "plt.ylim(0, 0.17)\n",
    "plt.title('Conversion rate by group', pad=20)\n",
    "plt.xlabel('Group', labelpad=15)\n",
    "plt.ylabel('Converted (proportion)', labelpad=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion rates for our groups are indeed very close. Also note that the conversion rate of the control group is lower than what we would have expected given what we knew about our avg. conversion rate (12.3% vs. 13%). This goes to show that there is some variation in results when sampling from a population.\n",
    "\n",
    "So… the treatment group's value is higher. Is this difference actually *statistically significant*?\n",
    "\n",
    "## 6. Testing the hypothesis\n",
    "The last step of our analysis is testing our hypothesis. Since we have a very large sample, we can use the normal (gaussian) approximation for calculating our p-value, i.e. we use the [z-test](https://en.wikipedia.org/wiki/Z-test).\n",
    "\n",
    "If we had assumed a student-T statistic, we would perform a [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test).\n",
    "\n",
    "We use the `statsmodels.stats.proportion` module to get the p-value and confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest, proportion_confint\n",
    "\n",
    "control_results = ab_test[ab_test['group'] == 'control']['converted']\n",
    "treatment_results = ab_test[ab_test['group'] == 'treatment']['converted']\n",
    "n_con = control_results.count()\n",
    "n_treat = treatment_results.count()\n",
    "successes = [control_results.sum(), treatment_results.sum()]\n",
    "nobs = [n_con, n_treat]\n",
    "\n",
    "z_stat, pval = proportions_ztest(successes, nobs=nobs)\n",
    "(lower_con, lower_treat), (upper_con, upper_treat) = proportion_confint(successes, nobs=nobs, alpha=0.05)\n",
    "\n",
    "print(f'z statistic: {z_stat:.2f}')\n",
    "print(f'p-value: {pval:.3f}')\n",
    "print(f'ci 95% for control group: [{lower_con:.3f}, {upper_con:.3f}]')\n",
    "print(f'ci 95% for treatment group: [{lower_treat:.3f}, {upper_treat:.3f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Drawing frequentist conclusions\n",
    "Since our p-value$=0.732$ is way above our $α=0.05$ threshold, *we cannot reject the Null hypothesis $Hₒ$*, which means that our new design did not perform significantly different (let alone better) than our old one :-(\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/disapointed.gif\" width=300 />\n",
    "</center>\n",
    "\n",
    "Additionally, if we look at the confidence interval for the treatment group (\\[0.116, 0.135\\], or 11.6-13.5%) we notice that:\n",
    "\n",
    "- It includes our baseline value of 13% conversion rate\n",
    "- It does not include our target value of 15% (the 2% uplift we were aiming for)\n",
    "\n",
    "What this means is that it is more likely that the true conversion rate of the new design is similar to our baseline, rather than the 15% target we had hoped for. This is further proof that our new design is not likely to be an improvement on our old design, and that unfortunately we are back to the drawing board!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bayesian analysis\n",
    "Our last step is to perform a Bayesian analysis, whose goal is to make direct probability statements about the parameter of\n",
    "interest by *building a model first*, and then testimating from the model.\n",
    "\n",
    "The “***Bayesian way***” to compare models is to compute the **marginal likelihood** of each model $p(y | H_k)$, i.e. the probability of the observed data $y$ given the model, and to take their ratio in order to evaluate the **Bayes factor**.\n",
    "\n",
    "The marginal likelihood is just the normalizing constant of Bayes’ theorem. We can see this if we write Bayes’ theorem and make explicit the fact that all inferences are model-dependant:\n",
    "\n",
    "For each model $H_k$:\n",
    "$$P (p \\mid y, H_k ) = \\frac{P(y \\mid p, H_k) P(p \\mid H_k)}{P( y \\mid H_k)}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $y$ is the data\n",
    "\n",
    "- $p$ the parameters\n",
    "\n",
    "- $H_k$ is one model out of K competing models\n",
    "\n",
    ">**Note**: Usually when doing inference we do not need to compute the marginal likelihood (normalizing constant in the denominator above), and in fact when using the Metropolis algorithm we don't at all, so in practice we often compute the posterior up to a constant proportional factor. However, for model comparison and model averaging the marginal likelihood is an *important quantity*!\n",
    "\n",
    "The main objective here is to compare models to determine which one is more likely and by how much. This can be achieved using the **Bayes factor**:\n",
    "\n",
    "$$BF =  \\frac{p(y \\mid H_0)}{p(y \\mid H_1)}$$\n",
    "\n",
    "The ***larger*** the BF, the ***better*** the model in the numerator right above ($H_0$ in this example). To ease the interpretation of BFs, some authors have proposed tables with levels of support or strength, just a way to put numbers into words:\n",
    "\n",
    "- 1-3: anecdotal (Note *anecdotal* comes from the greek work [ανεκδοτο](https://www.vocabulary.com/dictionary/anecdote), which means *unpublished*, and also used in modern greek as the word for ***joke***! In other words, **not deserving serious consideration**)\n",
    "\n",
    "- 3-10: moderate\n",
    "\n",
    "- 10-30: strong\n",
    "\n",
    "- 30-100: very strong\n",
    "\n",
    "- 100: extreme\n",
    "\n",
    ">**Note**: Computing the marginal likelihood is, generally, a ***hard task*** because it’s an integral (or a sum for discrete variables) of a highly variable function over a high dimensional parameter space. In general this integral needs to be solved numerically using more or less sophisticated methods:\n",
    "\n",
    ">$$P(y \\mid H_k) = \\int_{p_k} P(y \\mid p_k, H_k) \\; P(p_k | H_k) \\; dp_k$$\n",
    "\n",
    ">Also, the marginal likelihood depends sensitively on the specified prior for the parameters in each model $P(p_k \\mid H_k)$\n",
    "\n",
    "> And that is why many university statistics classes teach classical inference, rather than Bayesian inference.\n",
    "\n",
    ">If you want to read more about the math in Bayesian inferencing, and how it's equivalent to [physical themrodynamics](https://en.wikipedia.org/wiki/Thermodynamics) (wow!), read no further than [here](https://arxiv.org/pdf/1706.01428.pdf).\n",
    "\n",
    ">Thankfully, we speak python and we have packages that can do the complex math for us!\n",
    "\n",
    "Sometimes (not here), instead of choosing one single model from a set of candidate models, **model averaging** is about getting one meta-model by averaging the candidate models. The Bayesian version of this weighs each model by its marginal posterior probability:\n",
    "\n",
    "$$p(\\theta \\mid y) = \\sum_{k=1}^K p(\\theta \\mid y, M_k) \\; p(M_k \\mid y)$$\n",
    "\n",
    "Model averaging is, for example, what the **random forest** algorithm does to the **decision tree** algorithm.\n",
    " \n",
    "\n",
    "On to a Bayesian analysis! We start with histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.converted == 0]), len(df[df.converted == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ab_test[ab_test.converted == 0]), len(ab_test[ab_test.converted == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversions of the control group (old UI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(control_results, bins='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversions of the treatment group (new UI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(treatment_results, bins='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(treatment_results), len(control_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_results.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group (control and treatment) has a certain probability to generate either a 0 or a 1.\n",
    "\n",
    "So if we want to build a Bayesian model, we need to come up with a pdf for the likelihood, which we already know is a Bernoulli, ***and a pdf for the `p` of the Bernoulli***! \n",
    "\n",
    "Which analytic pdf do we know that yields a probability (a shape between 0 and 1)? The [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) of course (and by now you also know that the Dirichlet distribution is the multivariate equivalent of the beta).\n",
    "\n",
    "Here's a beta that displays the pdf underlying 5 wins and 5 losses for MV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "x = np.linspace(0, 1, 100)\n",
    "plt.plot(x, sp.stats.beta(5, 5).pdf(x))\n",
    "plt.ylabel(\"Beta(2, 8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 wins and 8 losses (the winning probability of a bad driver, definitely not MV!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, sp.stats.beta(2, 8).pdf(x))\n",
    "plt.ylabel(\"Beta(2, 8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 wins and 2 losses (the winning probability of an MV or a LH):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, sp.stats.beta(8, 2).pdf(x))\n",
    "plt.ylabel(\"Beta(2, 8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice something else very cool: Students always ask, why do we use the Beta as the pdf for the most uniformative priors? Looks to me we should use the uniform distribution, since it assigns equal probability to all outcomes.\n",
    "\n",
    "Well, here's the beta as a uniform distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, sp.stats.beta(1, 1).pdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How cool!\n",
    "\n",
    "Now, if we were to draw samples from the parameterized binomial distribution, where each sample is equal to the number of wins over n races (the third parameter says how many trials of the model you want to run, where each model trial is 100 trials of a binomial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result of racing 100 times, tested 1000 times,for a bad and good driver:\n",
    "n = 100  # number of trials\n",
    "A = np.random.binomial(n, 0.10, 1000) #a bad driver, with p=10% probability of winning\n",
    "B = np.random.binomial(n, 0.90, 1000) #a good driver, like LH, withp=90% probability of winning\n",
    "A, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s do 20,000 trials of the model, and count the number that generate 10 wins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.random.binomial(100, 0.10, 20000) == 10)/20000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's 13.6% instances of 10 wins.\n",
    "\n",
    "How about 3 and 20 wins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.random.binomial(100, 0.10, 20000) == 3)/20000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.random.binomial(100, 0.10, 20000) == 20)/20000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that much higher probability to observe 10 wins in 100 races for a driver with a 10% probability of winning. Makes good sense, right?\n",
    "\n",
    "## ~~Sampyl~~ (skip this section)\n",
    "Let's try an mcmc sim with `sampyl`. I don't think sampyl has a bernoulli or beta defined (I found its source code in [github](https://github.com/mcleonard/sampyl), ao I write my own bernoulli and beta functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sampyl as smp\n",
    "from sampyl import np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sampyl import bound\n",
    "alpha, beta = 2, 2\n",
    "N = 4720\n",
    "def logp(pA, pB):\n",
    "    # Likelihoods for A and B\n",
    "    #likelihood = lambda x, px: smp.bound(x*np.log(px) + \n",
    "    #                                 (N - x)*np.log(1 - px),\n",
    "    #                                 0 < px < 1)\n",
    "    likelihood = lambda x, px: x*np.log(px) + (N - x)*np.log(1 - px)\n",
    "    \n",
    "    likelihood_A = likelihood(A, pA)\n",
    "    likelihood_B = likelihood(B, pB)\n",
    "    \n",
    "    # Beta priors over A and B\n",
    "    #beta_prior = lambda px: bound((alpha - 1)*np.log(px) + \n",
    "    #                              (beta - 1)*np.log(1 - px),\n",
    "    #                              0 < px < 1)\n",
    "    beta_prior = lambda px: (alpha - 1)*np.log(px) +  (beta - 1)*np.log(1 - px)\n",
    "    \n",
    "    prior_A = beta_prior(pA)\n",
    "    prior_B = beta_prior(pB)\n",
    "    \n",
    "    return likelihood_A + likelihood_B + prior_A + prior_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts = smp.NUTS(logp, start={'pA': 0.1, 'pB': 0.1})\n",
    "trace = nuts.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... oopsie?\n",
    "\n",
    "On further source code investigation, sampyl *does* define a bernoulli :-) but not a beta :-( Let's try with an exponential instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log-P function here\n",
    "def logp(p1, p2):\n",
    "    model = smp.Model()\n",
    "    # Poisson log-likelihoods\n",
    "    model.add(smp.bernoulli(control_results.values, p=p1),\n",
    "              smp.bernoulli(treatment_results.values, p=p2))\n",
    "\n",
    "    # Exponential log-priors for p parameters\n",
    "    model.add(smp.exponential(p1),\n",
    "              smp.exponential(p2))\n",
    "    \n",
    "    return model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start  =  smp.find_MAP(logp, {'p1':0.5, 'p2':0.5})\n",
    "#start  =  {'p1':0.5, 'p2':0.5}\n",
    "#sampler = smp.Metropolis(logp, start)\n",
    "#chain = sampler(10000, burn=2000, thin=4)\n",
    "\n",
    "sampler = smp.Slice(logp, {'p1':2., 'p2':2.})\n",
    "chain = sampler.sample(20000, burn=4000, thin=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... Can someone fix this? I don't know `sampyl` as well as `pymc3`...\n",
    "\n",
    "## PyMC3\n",
    "Note that i've had to downground my `arviz` module from some previous misadventures. Fyi, i've had to uninstall then reinstall version 0.10.0 of `arviz` for ***my*** version of `pymc3`:\n",
    "```\n",
    "pip uninstall arviz\n",
    "pip install arviz==0.10.0\n",
    "```\n",
    "But don't do this, *you* may not have a versioning issue!\n",
    "\n",
    "So let's build our model: We set the data likelihoods for the control and treatment group to Bernoulli pdfs, and the single parameter of the Bernoulli (the \"*win*\" probability) needs to become a pdf, too. SInce it needs to be between 0 and 1, the only analytic pdf I know (which I also know is used a lot in sports analytics to evaluate talent based on wins and losses) is the **beta** function! It has two parameters: wins and losses!\n",
    "\n",
    "Let's set our initial parameters to something very *uninformative*: 2 \"wins\" and 2 \"losses\" for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, sp.stats.beta(2, 2).pdf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "#import arviz as az\n",
    "\n",
    "with pm.Model() as ui_model: \n",
    "    # define the priors \n",
    "    p_control = pm.Beta('p_control', 2, 2) \n",
    "    p_treatment = pm.Beta('p_treatment', 2, 2) \n",
    "    \n",
    "    # define the likelihoods \n",
    "    control_data = pm.Bernoulli('control_data', p_control, observed = control_results.values)\n",
    "    treatment_data = pm.Bernoulli('treatment_data', p_treatment, observed = treatment_results.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a **deterministic** parameter made out of the difference between the two probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Deterministic\n",
    "    \n",
    "with ui_model:\n",
    "    diff_of_p = Deterministic('difference of p', p_treatment - p_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ui_model: \n",
    "\n",
    "    # get the samples \n",
    "    #trace = pm.sample(return_inferencedata = True)\n",
    "    trace = pm.sample() #if this fries your kernel, then set cores=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trace.p_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trace.p_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trace.p_control, bins=30)\n",
    "plt.hist(trace.p_treatment, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the generated pdfs for each parameter are slightly different.\n",
    "\n",
    "We see that the treatment group has slightly higher probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pymc3 import plot_posterior\n",
    "#plot_posterior(trace[500:], varnames=['theta'], color='#87ceeb');\n",
    "\n",
    "import arviz as az\n",
    "az.plot_trace(trace, var_names=['p_control', 'p_treatment', 'difference of p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So simulations of our model *do* show a tendency to different probabilities for our treatment and control groups: The treatment group that sees the new UI is slightly better at creating conversions but that difference is *really small*, like less than 0.005, i.e. less than a half percentage point!\n",
    "\n",
    "Let's see if there's a difference when we take the entire populations into account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_data = df[df['group'] == 'control']['converted']\n",
    "treatment_data = df[df['group'] == 'treatment']['converted']\n",
    "len(control_data), len(treatment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as ui_model_data: \n",
    "    # define the priors \n",
    "    p_control = pm.Beta('p_control', 2, 2) \n",
    "    p_treatment = pm.Beta('p_treatment', 2, 2) \n",
    "    \n",
    "    # define the likelihoods \n",
    "    control_data = pm.Bernoulli('control_data', p_control, observed = control_data.values)\n",
    "    treatment_data = pm.Bernoulli('treatment_data', p_treatment, observed = treatment_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Deterministic\n",
    "    \n",
    "with ui_model_data:\n",
    "    diff_of_p = Deterministic('difference of p', p_treatment - p_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ui_model_data: \n",
    "\n",
    "    # get the samples \n",
    "    #trace = pm.sample(return_inferencedata = True)\n",
    "    trace2 = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trace2.p_control, bins=30)\n",
    "plt.hist(trace2.p_treatment, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the standard deviation is smaller (because we have *more information* thus better knowledge in bayesian language), and that the difference between the two models is sharper. But that does not mean there is a bigger effect? It could be that the reason we see a sharper difference is because we have tighter distributions.\n",
    "\n",
    ">**Note**: Oh my, the treatment group is now behind the control group in terms of probabilities to convert browsing users to a buy!\n",
    "\n",
    "So we look at the parameter pdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace2, var_names=['p_control', 'p_treatment', 'difference of p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, it looks like the difference between the two probabilities is very small, indeed smaller, less than a tenth of a percentage point. Interestingly though, it does look like the treatment group fares worse than the control group :-(\n",
    "\n",
    "`arviz` gives us results straight up with the `summary` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace2, var_names=['p_control', 'p_treatment'], kind=\"stats\").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty identical, we have the same mean value for `p` for both models, the standard deviation is very small for both (in fact it underflows). So the Bayesian conclusion is the same as the frequentist one: Not enough evidence for a difference in conversaion rates between the new and old UIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes factor\n",
    "We can also look at the Bayes factor: The ratio of the posterior probabilities of the two hypotheses:\n",
    "\n",
    "$$BF = \\frac{P(Y | H_0)}{P(Y | H_1)}$$\n",
    "\n",
    "The Bayes factor is not probability itself but a ratio of probabilities, ranging from zero to infinity. It has a more *objective* interpretation than p-values.\n",
    "\n",
    "For example, a Bayes factor of 0.5 simply indicates that the results we observe have half the probability under the null hypothesis as they are under the alternative hypothesis. The Bayes factor relies only on the observed data at hand, and not on some hypothetical repeated sampling, which we do not observe and are the essence of the calculation of the p-value. \n",
    "\n",
    "How to interpret the Bayes factor?\n",
    "- Bayes factor of 1 indicates no evidence (i.e. equal support for both hypotheses).\n",
    "- Bayes factor between 1 and 3 indicates anecdotal evidence for H0.\n",
    "- Bayes factor between 3 and 10 indicates substantial evidence for H0.\n",
    "- Bayes factor between 1/3 and 1 indicates anecdotal evidence for H1.\n",
    "- Bayes factor between 1/10 and 1/3 indicates substantial evidence for H1. \n",
    "\n",
    "There is a difference between the Bayes factor (BF) and the **Posterior Odds Ratio** (POR). The POR\n",
    "is the BF multiplied by the ratio of prior probabilities that we attach to the two hypotheses. \n",
    "\n",
    "Since we rarely have information to come up with a reasonable prior odds ratio, we set it to unity (so\n",
    "that the two hypotheses receive the same prior probability) and therefore the BF is simply the\n",
    "POR. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model here is:\n",
    "\n",
    "$$p \\sim Beta(\\alpha, \\beta)$$\n",
    "\n",
    "$$y \\sim Bernoulli(k=p) \\sim Binomial(n=1, k=p)$$\n",
    "\n",
    "Mathematicians tell us that in this case, the marginal likelihood can be analytically derived:\n",
    "\n",
    "$$p(y) = \\binom {n}{h}  \\frac{B(\\alpha + h,\\ \\beta + n - h)} {B(\\alpha, \\beta)}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $B$ is the [beta function](https://en.wikipedia.org/wiki/Beta_function). not to be confused with the [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution)\n",
    "\n",
    "- $n$ is the number of trials (user sessions)\n",
    "\n",
    "- $h$ is the number of successes (conversions)\n",
    "\n",
    "Since we only care about the relative value of the marginal likelihood under two different models (for the same data), we can omit the binomial coefficient $\\binom {n}{h}$:\n",
    "\n",
    "$$p(y) \\propto \\frac{B(\\alpha + h,\\ \\beta + n - h)} {B(\\alpha, \\beta)}$$\n",
    "\n",
    "Let's code it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import betaln # use the betaln function instead of the beta function, to prevent underflow\n",
    "from scipy.stats import beta\n",
    "print(f\"Running on PyMC3 v{pm.__version__}\")\n",
    "\n",
    "def beta_binom(prior, y):\n",
    "    \"\"\"\n",
    "    Compute the marginal likelihood, analytically, for a beta-binomial model.\n",
    "\n",
    "    prior : tuple\n",
    "        tuple of alpha and beta parameter for the prior (beta distribution)\n",
    "    y : array\n",
    "        array with \"1\" and \"0\" corresponding to the success and fails respectively\n",
    "    \"\"\"\n",
    "    alpha, beta = prior\n",
    "    h = np.sum(y)\n",
    "    n = len(y)\n",
    "    p_y = np.exp(betaln(alpha + h, beta + n - h) - betaln(alpha, beta))\n",
    "    return p_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "Let's warm up to get a better feel for the Bayes factor: Let's flip a coin 100 times with the same number of observed *heads* and *tails*. \n",
    "\n",
    "Let's compare two models, one with a uniform prior, and one with a more concentrated prior around 0.5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.repeat([1, 0], [50, 50])  # 50 \"heads\" and 50 \"tails\"\n",
    "priors = ((1, 1), (30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the two marginal likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in priors:\n",
    "    distri = beta(a, b)\n",
    "    x = np.linspace(0, 1, 300)\n",
    "    x_pdf = distri.pdf(x)\n",
    "    plt.plot(x, x_pdf, label=fr\"$\\alpha$ = {a:d}, $\\beta$ = {b:d}\")\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"$\\\\theta$\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the Bayes factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF = beta_binom(priors[1], y) / beta_binom(priors[0], y)\n",
    "print(round(BF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model with the more concentrated prior has 5 times more support than the model with the more extended prior and thus indicates **substantial evidence** for the model with the more concentrated prior. It's a better model!\n",
    "\n",
    "Besides the exact numerical value this should not be surprising since the prior for the most favoured model is concentrated around 0.5 and the data $y$ has equal number of head and tails, consistent with a value of $p$ around 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to our conversion data\n",
    "Our simulations yielded the posterior, or most probable beta distributions for the single parameter of our model, *but not the parameters* $\\alpha, \\beta$ of the beta distribution, which we need to evaluate the marginal likelihood and the Bayes factor. Oh no!\n",
    "\n",
    "But I can evaluate these parameters from the empirical mean and variance of our simulated data!\n",
    "\n",
    "Specifically, the theoretical mean and variance for a beta function are:\n",
    "\n",
    "$$\\mu=\\frac{\\alpha}{\\alpha+\\beta}$$\n",
    "\n",
    "$$\\sigma^2=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$\n",
    "\n",
    "Solving (can someone please verify?):\n",
    "\n",
    "$$\\alpha=\\left(\\frac{1-\\mu}{\\sigma^2}-\\frac{1}{\\mu}\\right)\\mu^2$$\n",
    "\n",
    "$$\\beta=\\alpha\\left(\\frac{1}{\\mu}-1\\right)$$\n",
    "\n",
    "Also, the bounds of $μ$ and $σ^2$ for any given Beta distribution:\n",
    "\n",
    "$$\\mu=\\frac{\\alpha}{\\alpha+\\beta}\\in\\left(0, 1\\right)$$\n",
    "\n",
    "$$\\sigma^2=\\frac{\\alpha\\beta}{\\left(\\alpha+\\beta\\right)^2\\left(\\alpha+\\beta+1\\right)}=\\frac{\\mu\\left(1-\\mu\\right)}{\\alpha+\\beta+1}<\\frac{\\mu\\left(1-\\mu\\right)}{1}=\\mu\\left(1-\\mu\\right)\\in\\left(0,0.5^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equating theoretical and empirical point estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_beta(mu, var):\n",
    "    alpha = ((1 - mu) / var - 1 / mu) * mu ** 2\n",
    "    beta = alpha * (1 / mu - 1)\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The control beta posterior's mean and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.p_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_control = np.mean(trace.p_control)\n",
    "mu_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_control = np.var(trace.p_control)\n",
    "var_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The treatment beta posterior's mean and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_treatment = np.mean(trace.p_treatment)\n",
    "mu_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_treatment = np.var(trace.p_treatment)\n",
    "var_treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the control beta posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_control, beta_control = alpha_beta(mu_control, var_control)\n",
    "alpha_control, beta_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the treatment beta posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_treatment, beta_treatment = alpha_beta(mu_treatment, var_treatment)\n",
    "alpha_treatment, beta_treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can finally compute the Bayes factor of our null and alternative hypotheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_y = df[df['group'] == 'control']['converted']\n",
    "treatment_y = df[df['group'] == 'treatment']['converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.nonzero(control_y.values)[0]), len(control_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.nonzero(treatment_y.values)[0]), len(treatment_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_binom((alpha_treatment, beta_treatment), control_y.values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oops, we underflow :-(\n",
    "\n",
    "Let's renormalize by dividing by 1,000 (I'm not sure I'm statistically allowed to do that, but I do it anyway ;-):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.sum(treatment_y.values)\n",
    "n = len(treatment_y.values)\n",
    "ml_treatment = np.exp(\n",
    "    betaln(alpha_treatment/1000 + h/1000, beta_treatment/1000 + n/1000 - h/1000) - \n",
    "    betaln(alpha_treatment/1000, beta_treatment/1000))\n",
    "ml_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.sum(control_y.values)\n",
    "n = len(control_y.values)\n",
    "ml_control = np.exp(\n",
    "    betaln(alpha_control/1000 + h/1000, beta_control/1000 + n/1000 - h/1000) - \n",
    "    betaln(alpha_control/1000, beta_control/1000))\n",
    "ml_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF = ml_treatment / ml_control\n",
    "print(round(BF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that a Bayes factor between 1 and 3 indicates **anecdotal evidence** for the treatment group (the one in the numerator). So, Bayesian inference tells us that *the new Web page is slightly better in terms of increasing conversion rates, but not enough to effectively conclude that the new UI is better than the old one*.\n",
    "\n",
    "We need to repeat this calculation for our sampled data! It's usually not realistic to assume that we can sample all observations,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating from the posterior distribution\n",
    "Another viable option would be to use our generated model to generate data and conclude from the \"*fake data*\" rather than the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['group'] == 'control']['converted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our model, I sample from the posterior 100 times and thus *simulate* data from both the control and the treatment group 100 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppc_0 = pm.sample_posterior_predictive(trace, 1, ui_model_data, size=(1, 1))\n",
    "#ppc_0 = pm.sample_posterior_predictive(trace, 1, ui_model_data)\n",
    "ppc = pm.sample_posterior_predictive(trace, 100, ui_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc['control_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc['control_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc['control_data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(ppc['control_data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(ppc['control_data'][0])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ppc_0['treatment_data'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc_0['treatment_data'].T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ppc_0['treatment_data'].T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) is a non-parametric way to estimate the probability density function of a random variable. \n",
    "\n",
    "Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_treatment = []\n",
    "for m_0 in ppc_0['treatment_data'].T:\n",
    "    means_treatment.append(np.mean(m_0))\n",
    "    \n",
    "az.plot_kde(means_treatment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about the oscillations, they are caused by the fact that we have count data (integers) but plotting on the real axis with smoothing effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_control = []\n",
    "for m_0 in ppc_0['control_data'].T:\n",
    "    means_control.append(np.mean(m_0))\n",
    "    \n",
    "az.plot_kde(means_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(9, 6))\n",
    "az.plot_kde(means_control, ax=ax, plot_kwargs={\"color\": \"C0\"})\n",
    "az.plot_kde(means_treatment, ax=ax, plot_kwargs={\"color\": \"C1\"})\n",
    "ax.plot([], label=\"control\")\n",
    "ax.plot([], label=\"treatment\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"$p$\")\n",
    "ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get very similar predictions for the `p` parameter of our posterior beta distribution. \n",
    "\n",
    "Note that it is possible to have two different models that nevertheless also yield similar predictions. The reason is that the data is informative enough to reduce the effect of the prior up to the point of inducing a very similar posterior. As predictions are computed from the posterior we also get very similar predictions. \n",
    "\n",
    "In most scenarios when comparing models, what we really care is the predictive accuracy of the models, if two models have similar predictive accuracy we consider both models to be similar.\n",
    "\n",
    "Tools used to estimate predictive accuracy include [WAIC](https://en.wikipedia.org/wiki/Watanabe%E2%80%93Akaike_information_criterion), [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)), or [LOO-CV](http://mc-stan.org/loo/reference/loo-package.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now you know how to design statistical experiments*!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
