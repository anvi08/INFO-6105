{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Sci Eng Methods and Tools, Lecture 13 Day 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 27 November 2022, with material from Thomas Wiecki and Robert Kubler</div>\n",
    "\n",
    "# 1. Some model-building roadblocks\n",
    "### Presence of outliers\n",
    "A common situation in which robust estimation is used occurs when the data contain **outliers**. In the presence of outliers that do not come from the same data-generating process as the rest of the data, least squares estimation is ***inefficient*** and can be biased. \n",
    "\n",
    "Because the least squares predictions are dragged towards the outliers, and because the variance of the estimates is artificially inflated, the result is that outliers can be masked. \n",
    "\n",
    "How to deal with outliers so they don't skew our model too much?\n",
    "\n",
    "### Working with non-linear effects\n",
    "One can deal with non-linear effects by squaring or taking powers of the independent variables. For example, if you suspect that $y \\approx \\alpha x + \\beta x^2$, then just rename $x_2 = x^2$ as another variable, and we're back to a linear model (of two variables)!\n",
    "\n",
    "However, it is an ***unrealistically strong assumption*** that every single value of $y$, after transformation using sone kernel function (`log()`, `sqrt()` etc.), will end up having a linear relationship with $X$. So, instead of $y$, we use...\n",
    "\n",
    "\n",
    "# 2. Generalized Linear Models\n",
    "**General Linear Regression models** is how we do data science when we think there is a **linear** relationship between, maybe not a dependent variable directly, but ***one of the statistical moments of*** a dependent variable (e.g. the mean), and other independent variable(s).\n",
    "\n",
    "Here are the basics of linear models:\n",
    "\n",
    "- Additive relationships: Classical Linear models assume that the regression variables should have an additive relationship with each other.\n",
    "\n",
    "- Classical Linear models assume that the data should have *constant variance* i.e. the data should be [homoscedastic](https://en.wikipedia.org/wiki/Homoscedasticity). In real life, data is often *not* homoscedastic. The variance is not constant and sometimes it is a function of the mean. For e.g. the variance increases as the mean increases. This is common in financial datasets, for example. We also know that for some data likelihoods, the variance *should*, in fact, vrey with the mean (e.g. Poisson).\n",
    "\n",
    "- Classical Linear models assume the errors of regression, also known as the residuals (the residual for each observation is the difference between predicted values of the dependent variable and observed values of the dependent variable), are normally distributed with mean zero. This condition is difficult to meet in real life.\n",
    "\n",
    "- Non-correlated variables: Finally, the regression variables are assumed to be non-correlated with each other, and preferably independent of each other.\n",
    "\n",
    "If your dataset is [heteroscedastic](https://en.wikipedia.org/wiki/Heteroscedasticity), if the residuals are not normally distributed, which is often the case in real world data sets, one needs to apply a suitable transformation (a.k.a a kernel method) to both $y$ and $X$ so as to make the relationship **linear** and at the same time stabilize the variance and normalize the errors.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/homoscedastic.png\" width=600 />\n",
    "</left>\n",
    "\n",
    "The square root and the logarithm transformations are commonly used for achieving these effects:\n",
    "\n",
    "Here's a non-linear (polynomial, or power) relationship:\n",
    "\n",
    "$$ y = x_1^{\\beta_1} * x_2^{\\beta_2}* \\cdots * x_n^{\\beta_n}$$\n",
    "\n",
    "Then do this:\n",
    "\n",
    "$$ log(y) = \\beta_1 log(x_1) + \\beta_2 log(x_2) + \\cdots + \\beta_n log(x_n)$$\n",
    "\n",
    "Make $log(x_i)$ the independent variable, and, voila, you have a linear relationship!\n",
    "\n",
    "Suppose $y$ is a random variable that follows some kind of a probability distribution. So for any given combination of $X$ values in the data set, the real world is likely to present to you several random values of $y$ and only some of these possible values will appear in your training samples. In the real world, these values of $y$ will be randomly distributed around the conditional mean of $y$ given the specific value of $X$. The conditional mean of $y$ is denoted by $E(y|X)$.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/y-versus-x.png\" width=600 />\n",
    "</left>\n",
    "\n",
    "In the variable transformation approach, we make the unrealistically strong assumption that every single value of $y$ i.e. each one of the blue dots in the above plot, after transformation using log(), sqrt() etc., will end up having a linear relationship with $X$. This is obviously too much to expect.\n",
    "\n",
    "### A more realistic model\n",
    "It is an ***unrealistically strong assumption*** that every single value of $y$, after transformation using `log()`, `sqrt()` etc., will end up having a linear relationship with $X$:\n",
    "\n",
    "$$ y = \\beta_1 log(x_1) + \\beta_2 log(x_2) + \\cdots + \\beta_n log(x_n)$$\n",
    "\n",
    "What seems more realistic is that the conditional mean (a.k.a. **expectation**) of $y$, i.e. $E(y|x)$, after a suitable transformation, ought to have a linear relationship with $X$.\n",
    "\n",
    "The **conditional expectations** of $Y$ on $X = x_i$ is denoted by:\n",
    "\n",
    "$$\\pi_i = E(y|X=x_i)$$\n",
    "\n",
    "Instead of transforming every single value of $y$ for each $x$, GLMs transform only the conditional expectation of $y$ for each $x$. So there is no need to assume that every single value of $y$ is expressible as a linear combination of regression variables.\n",
    "\n",
    "$$ log(E(y|X=x_i)) = \\beta_1 log(x_1) + \\beta_2 log(x_2) + \\cdots + \\beta_n log(x_n)$$\n",
    "\n",
    "The transformation function (the kernel method) is called the **link function** of the GLM and is denoted by $g()$. In the above example, the log() is the link function, i.e. $g() = log()$.\n",
    "\n",
    "The link function $g()$ can take many forms and we get a different regression model based on what form $g()$ takes. Here are a few popular forms and the corresponding regression models that they lead to\n",
    "\n",
    "### Some link functions\n",
    "For a **linear** regression model:\n",
    "\n",
    "$$g(\\pi_i) = \\pi_i$$\n",
    "\n",
    "For a **logistic** (and binomial) regression model:\n",
    "\n",
    "$$g(\\pi_i) = \\frac{\\pi_i}{1 - \\pi_i}$$\n",
    "\n",
    "which means:\n",
    "\n",
    "$$\\pi_i = \\frac{e^{\\beta x_i}}{1 - e^{\\beta x_i}}$$\n",
    "\n",
    "For a Poisson model:\n",
    "\n",
    "$$g(\\pi_i) = ln(\\pi_i)$$\n",
    "\n",
    "which means:\n",
    "\n",
    "$$\\pi_i = e^{\\beta x_i}$$\n",
    "\n",
    "There are many other variants of $g()$ such as the Poisson-Gamma mixture leading to the Negative Binomial regression model and the inverse of the Cumulative Distribution Function of the Normal distribution, which leads to the probit model.\n",
    "\n",
    "### Handling  heteroscedasticity\n",
    "When your data data has variance which is is not constant or residual errors are non-normal, what to do?\n",
    "\n",
    "GLMs account for the possibility of a non-constant variance by assuming that the variance is some function $V(µ)$ of the mean $µ$, or more accurately the conditional mean $µ|X=x$.\n",
    "\n",
    "In each of the above mentioned models, we assume a suitable variance function $V(µ|X=x)$.\n",
    "\n",
    ">In Generalized Linear Models, one expresses the variance in the data as a suitable function of the mean value.\n",
    "\n",
    "In the Linear regression model, we assume $V(µ)$ = some constant, i.e. variance is constant. Why? Because Linear models assume that $y$ is Normally distributed and a Normal distribution has a constant variance.\n",
    "\n",
    "In the Logistic and Binomial Regression models, we assume, $V(µ) = µ — µ²/n$ for a data set size of $n$ samples, as required by a Logit-distributed $y$ value.\n",
    "\n",
    "In the Poisson Regression model, we assume $V(µ) = µ$. This is because, the Poisson regression model assumes that $y$ has a Poisson distribution and in a Poisson distribution, variance = mean.\n",
    "\n",
    "In the Negative Binomial regression model, we assume $V(µ) = µ + α*µ²$, where $α$ is a dispersion parameter which allows us to deal with over-dispersed or under-dispersed data.\n",
    "\n",
    "…and so on for other models.\n",
    "\n",
    "In GLMs, it is possible to show that the model is not sensitive to the distributional form of the residual errors. In simple terms, the model doesn’t care whether the model’s errors are normally distributed or distributed any other way, as long as the mean-variance relationship that you assume, is actually satisfied by your data.\n",
    "\n",
    ">Generalized Linear Models do not care if the residual errors are normally distributed or not as long as the specified mean-variance relationship is satisfied by the data.\n",
    "\n",
    "This makes GLMs a practical choice for many real world data sets that are nonlinear and heteroscedastic and in which we cannot assume that the model’s errors will always be normally distributed.\n",
    "\n",
    "Finally, a word of caution: Similar to Classical Linear Regression models, GLMs also assume that the regression variables are uncorrelated with each other. Therefore GLMs cannot be used to model time series data which typically contain a lot of auto-correlated observations.\n",
    "\n",
    ">Generalized Linear Models should not be used for modeling auto-correlated time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple Linear Regression in an imaginary World\n",
    "Frequentist statistics have different tests for different scenarios (T test, chi-squared, etc.). \n",
    "\n",
    "In Bayesian land you define your model by an equation for the data likelihiid, equate its variables to pdfs, and simulate with an MCMC sampling algorithm like `PyMC3`'s Metroplois.\n",
    "\n",
    "## Frequentist formulation\n",
    "In general, frequentists think about Linear Regression as follows:\n",
    "\n",
    "$$Y=βX+ϵ$$\n",
    "\n",
    "where $Y$ is the output we want to predict (or dependent variable), $X$ is our predictor (or independent variable), and $β$ are the coefficients (or parameters) of the model we want to estimate. $ϵ$ is an error term which is usually assumed to be normally distributed.\n",
    "\n",
    "We can then use Ordinary Least Squares (OLS) or Maximum Likelihood (MLE) to find the best fitting $β$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian reformulation\n",
    "Bayesians take a probabilistic view of the world and express this model in terms of probability distributions. Our above linear regression can be rewritten:\n",
    "\n",
    "$Y∼N(Xβ,σ^2)$\n",
    "\n",
    "In words, we view $Y$ as a random variable (or random vector) of which each element (data point) is distributed according to a **Normal distribution**. \n",
    "\n",
    "The mean of this normal distribution is provided by our linear predictor, with variance $σ^2$.\n",
    "\n",
    "There are two critical advantages of Bayesian estimation:\n",
    "\n",
    "- **Priors**: We can quantify any prior knowledge we might have by placing priors on the parameters. For example, if we think that $σ$ is likely to be small we would choose a prior with more probability mass on low values.\n",
    "- **Quantifying uncertainty**: We do not get a single estimate of $β$ as above but instead a complete **posterior distribution** about how likely different values of $β$ are. For example, with few data points our uncertainty in $β$ will be very high and we'd be getting very wide posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pymc3 import  *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data\n",
    "Let's create some toy data to play around with and scatter-plot it.\n",
    "\n",
    "We create a regression line defined by intercept and slope, and add data points with noise sampled from a Normal with the mean set to the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 200\n",
    "true_intercept = 1\n",
    "true_slope = 2\n",
    "\n",
    "x = np.linspace(0, 1, size)\n",
    "# y = a + b*x\n",
    "true_regression_line = true_intercept + true_slope * x\n",
    "\n",
    "# add noise\n",
    "y = true_regression_line + np.random.normal(scale=.5, size=size)\n",
    "\n",
    "data = dict(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')\n",
    "ax.plot(x, y, 'x', label='sampled data')\n",
    "ax.plot(x, true_regression_line, label='true regression line', lw=2.)\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian simulation\n",
    "Lets fit a Bayesian linear regression model to this data.\n",
    "\n",
    "The [Cauchy distribution](https://en.wikipedia.org/wiki/Cauchy_distribution) and [Half-Cauchy distribution](https://distribution-explorer.github.io/continuous/halfcauchy.html) are often used as a prior model for the noise.\n",
    "\n",
    "We haven't used these before, so they're new disributions for us!\n",
    "\n",
    ">I found a really neat [distribution explorer](https://distribution-explorer.github.io) that shows you all the important model-building distributions for dataset histograms.\n",
    "\n",
    "The [Half-Cauchy distribution](https://distribution-explorer.github.io/continuous/halfcauchy.html) is a Cauchy distribution truncated to only have nonzero probability density for values greater than or equal to the location of the peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hamiltonian Monte Carlo\n",
    "#weaking the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\deprecat\\classic.py:215: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  return wrapped_(*args_, **kwargs_)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\theano\\tensor\\elemwise.py:826: RuntimeWarning: divide by zero encountered in log\n",
      "  variables = ufunc(*ufunc_args, **ufunc_kwargs)\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\theano\\tensor\\elemwise.py:826: RuntimeWarning: invalid value encountered in multiply\n",
      "  variables = ufunc(*ufunc_args, **ufunc_kwargs)\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [x, Intercept, sigma]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='4000' class='' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [4000/4000 11:51&lt;00:00 Sampling 2 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 728 seconds.\n"
     ]
    }
   ],
   "source": [
    "with Model() as model: # model specifications in PyMC3 are wrapped in a with-statement\n",
    "    # Define priors\n",
    "    sigma = HalfCauchy('sigma', beta=10, testval=1.)\n",
    "    intercept = Normal('Intercept', 0, sd=20)\n",
    "    x_coeff = Normal('x', 0, sd=20)\n",
    "    \n",
    "    # Define likelihood\n",
    "    likelihood = Normal('y', mu=intercept + x_coeff * x, \n",
    "                        sd=sigma, observed=y)\n",
    "    \n",
    "    # Inference!\n",
    "    trace = sample(progressbar=True) # draw posterior samples using NUTS sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, `PyMC3` has a more Dua Lipa formulation that I introduced you to last week. Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The glm module is deprecated and will be removed in version 4.0\n",
      "We recommend to instead use Bambi https://bambinos.github.io/bambi/\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\deprecat\\classic.py:215: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  return wrapped_(*args_, **kwargs_)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\theano\\tensor\\elemwise.py:826: RuntimeWarning: divide by zero encountered in log\n",
      "  variables = ufunc(*ufunc_args, **ufunc_kwargs)\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\theano\\tensor\\elemwise.py:826: RuntimeWarning: invalid value encountered in multiply\n",
      "  variables = ufunc(*ufunc_args, **ufunc_kwargs)\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [sd, x, Intercept]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8000/8000 09:49&lt;00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 623 seconds.\n"
     ]
    }
   ],
   "source": [
    "with Model() as model:\n",
    "    # specify glm and pass in data. The resulting linear model, its likelihood and \n",
    "    # and all its parameters are automatically added to our model.\n",
    "    GLM.from_formula('y ~ x', data)\n",
    "    trace = sample(progressbar=True, tune=1000, cores=4) # draw posterior samples using NUTS sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much shorter, but this code does the exact same thing as the above model specification (you can change priors and everything else too if you wanted, I show you later how to). \n",
    "\n",
    "`glm()` parses the Patsy model string, adds random variables for each regressor (Intercept and slope x in this case), adds a likelihood (by default, a Normal is chosen), and all other variables (sigma). \n",
    "\n",
    "Finally, `glm()` initializes the parameters to a good starting point by estimating a frequentist linear model using statsmodels.\n",
    "\n",
    "**Patsy syntax**, `y ~ x` specifies that we have an output variable y that we want to estimate as a linear function of x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis\n",
    "Bayesian inference does not give us only one best fitting line (as maximum likelihood does) but rather a whole posterior distribution of likely parameters. Lets plot the posterior distribution of our parameters and the individual samples we drew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "traceplot(trace)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left side shows our **marginal posterior** -- for each parameter value on the x-axis we get a probability on the y-axis that tells us how likely that parameter value is.\n",
    "\n",
    "There are a couple of things to see here. The first is that our sampling chains for the individual parameters (left side) seem well converged and stationary (there are no large drifts or other odd patterns). We see 4 lines because we ran 4 chains in parallel (`cores` parameter above). They all seem to provide the same answer which is re-assuring.\n",
    "\n",
    "Secondly, the maximum posterior estimate of each variable (the peak in the left side distributions) is very close to the true parameters used to generate the data (x is the regression coefficient and sigma is the standard deviation of our normal).\n",
    "\n",
    "In the GLM we thus do not only have one best fitting regression line, but many. A posterior predictive plot takes multiple samples from the posterior (intercepts and slopes) and plots a regression line for each of them. Here we are using the `glm.plot_posterior_predictive()` convenience function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'x', label='data')\n",
    "plots.plot_posterior_predictive_glm(trace, samples=100, \n",
    "                                    label='posterior predictive regression lines')\n",
    "plt.plot(x, true_regression_line, label='true regression line', lw=3., c='y')\n",
    "\n",
    "plt.title('Posterior predictive regression lines')\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our estimated regression lines are very similar to the true regression line. But since we only have limited data we have uncertainty in our estimates, here expressed by the variability of the lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The real World has outliers\n",
    "The problem with simulated data though is that it's ***simulated***. In the real world things tend to get more messy and assumptions like normality are easily violated by a few **outliers**.\n",
    "\n",
    "Lets see what happens if we add some outliers to our simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 200\n",
    "true_intercept = 1\n",
    "true_slope = 2\n",
    "\n",
    "x = np.linspace(0, 1, size)\n",
    "# y = a + b*x\n",
    "true_regression_line = true_intercept + true_slope * x\n",
    "\n",
    "# add noise\n",
    "y = true_regression_line + np.random.normal(scale=.5, size=size)\n",
    "\n",
    "# Add outliers\n",
    "x_out = np.append(x, [.1, .15, .2])\n",
    "y_out = np.append(y, [8, 6, 9])\n",
    "\n",
    "data = dict(x=x_out, y=y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')\n",
    "ax.plot(x_out, y_out, 'x', label='sampled data')\n",
    "ax.plot(x, true_regression_line, label='true regression line', lw=2.)\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what happens if we estimate our Bayesian linear regression model using the `glm()` function as before. This function takes a Patsy string to describe the linear model and adds a Normal likelihood by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Model() as model:\n",
    "    GLM.from_formula('y ~ x', data)\n",
    "    trace = sample(progressbar=True, tune=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the posterior predictive regression lines by taking regression parameters from the posterior distribution and plotting a regression line for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(111, xlabel='x', ylabel='y', \n",
    "            title='Posterior predictive regression lines')\n",
    "plt.plot(x_out, y_out, 'x', label='data')\n",
    "\n",
    "# plots is a PyMC3 package\n",
    "plots.plot_posterior_predictive_glm(trace, samples=100, \n",
    "                                 label='posterior predictive regression lines')\n",
    "plt.plot(x, true_regression_line, \n",
    "         label='true regression line', lw=3., c='y')\n",
    "\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit is quite skewed and we have a fair amount of uncertainty in our estimate as indicated by the wide range of different posterior predictive regression lines. \n",
    "\n",
    "Why is this? The reason is that the normal distribution ***does not have a lot of mass in the tails*** and consequently is unfit to model data with a lot (or a few very distant) outliers.\n",
    "\n",
    "Since the problem is the light tails of the Normal distribution, we can instead assume that our data is not normally distributed but instead distributed according to the **Student-T** distribution which has heavier tails.\n",
    "\n",
    "With `theano`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Normal, StudentT\n",
    "normal_dist = Normal.dist(mu=0, sd=1)\n",
    "t_dist = StudentT.dist(mu=0, lam=1, nu=1)\n",
    "\n",
    "import theano\n",
    "x_eval = np.linspace(-8, 8, 300)\n",
    "plt.plot(x_eval, theano.tensor.exp(normal_dist.logp(x_eval)).eval(), label='Normal', lw=2.)\n",
    "plt.plot(x_eval, theano.tensor.exp(t_dist.logp(x_eval)).eval(), label='Student T', lw=2.)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability density')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `pm.math`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Normal, StudentT\n",
    "normal_dist = Normal.dist(mu=0, sd=1)\n",
    "t_dist = StudentT.dist(mu=0, lam=1, nu=1)\n",
    "\n",
    "x_eval = np.linspace(-8, 8, 300)\n",
    "plt.plot(x_eval, pm.math.exp(normal_dist.logp(x_eval)).eval(), label='Normal', lw=2.)\n",
    "plt.plot(x_eval, pm.math.exp(t_dist.logp(x_eval)).eval(), label='Student T', lw=2.)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability density')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the usage of a Student-T distribution in PyMC3 (to specify that our data is Student T-distributed), we do the following.\n",
    "\n",
    "The Student-T distribution has, besides the mean and variance, a third parameter called **degrees of freedom** that describes how much mass should be put into the tails. Here it is set to 1 which gives maximum mass to the tails (setting this to infinity results in a Normal distribution!). One could also place a prior on this rather than fixing it.\n",
    "\n",
    "See `glm.families` for more choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Model() as model_robust:\n",
    "    family = glm.families.StudentT()\n",
    "    GLM.from_formula('y ~ x', data, family=family)\n",
    "    trace_robust = sample(progressbar=True, tune=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.plot(x_out, y_out, 'x')\n",
    "\n",
    "plots.plot_posterior_predictive_glm(trace_robust, label='posterior predictive regression lines')\n",
    "plt.plot(x, true_regression_line, \n",
    "         label='true regression line', lw=3., c='y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we see that the outliers are barely influencing our estimation at all because our likelihood function assumes that outliers are much more probable than under the Normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hierarchical Linear regression\n",
    "Gelman et al.'s (2007) radon dataset is a classic dataset in data science. \n",
    "\n",
    "In this dataset the amount of the radioactive gas radon has been measured among different households in all county's of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to enter the house through the basement. Moreover, its concentration is thought to differ regionally due to different types of soil.\n",
    "\n",
    "Let's investigate this difference and  make predictions of radon levels in different counties, *and also where in the house radon was measured*. \n",
    "\n",
    "Let's look at the state Minnesota, a state that contains 85 countiess in which different measurements are taken, ranging from 2 till 80 measurements per county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc3 as pm \n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/radon.csv')\n",
    "\n",
    "county_names = data.county.unique()\n",
    "county_idx = data['county_code'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>log_radon</th>\n",
       "      <th>floor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AITKIN</td>\n",
       "      <td>0.832909</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AITKIN</td>\n",
       "      <td>0.832909</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AITKIN</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AITKIN</td>\n",
       "      <td>0.095310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANOKA</td>\n",
       "      <td>1.163151</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANOKA</td>\n",
       "      <td>0.955511</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ANOKA</td>\n",
       "      <td>0.470004</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANOKA</td>\n",
       "      <td>0.095310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANOKA</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ANOKA</td>\n",
       "      <td>0.262364</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   county  log_radon  floor\n",
       "0  AITKIN   0.832909    1.0\n",
       "1  AITKIN   0.832909    0.0\n",
       "2  AITKIN   1.098612    0.0\n",
       "3  AITKIN   0.095310    0.0\n",
       "4   ANOKA   1.163151    0.0\n",
       "5   ANOKA   0.955511    0.0\n",
       "6   ANOKA   0.470004    0.0\n",
       "7   ANOKA   0.095310    0.0\n",
       "8   ANOKA  -0.223144    0.0\n",
       "9   ANOKA   0.262364    0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['county', 'log_radon', 'floor']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>log_radon</th>\n",
       "      <th>floor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>WRIGHT</td>\n",
       "      <td>1.648659</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>WRIGHT</td>\n",
       "      <td>0.832909</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>WRIGHT</td>\n",
       "      <td>0.875469</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>WRIGHT</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>WRIGHT</td>\n",
       "      <td>2.261763</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>WRIGHT</td>\n",
       "      <td>1.871802</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>WRIGHT</td>\n",
       "      <td>1.526056</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>WRIGHT</td>\n",
       "      <td>1.629241</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>YELLOW MEDICINE</td>\n",
       "      <td>1.335001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>YELLOW MEDICINE</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              county  log_radon  floor\n",
       "909           WRIGHT   1.648659    0.0\n",
       "910           WRIGHT   0.832909    0.0\n",
       "911           WRIGHT   0.875469    1.0\n",
       "912           WRIGHT   2.772589    0.0\n",
       "913           WRIGHT   2.261763    0.0\n",
       "914           WRIGHT   1.871802    0.0\n",
       "915           WRIGHT   1.526056    0.0\n",
       "916           WRIGHT   1.629241    0.0\n",
       "917  YELLOW MEDICINE   1.335001    0.0\n",
       "918  YELLOW MEDICINE   1.098612    0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['county', 'log_radon', 'floor']].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a past notebook, we drew the histogram of the data so we could match it to a well-known statistical function.\n",
    "\n",
    "Seaborn's `distplot` computes the histogram *and also fits it to a curve*. See [here](https://seaborn.pydata.org/generated/seaborn.distplot.html) for the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='log_radon', ylabel='Density'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "\n",
    "hennepin_radon = data.query('county==\"HENNEPIN\"').log_radon\n",
    "sns.distplot(hennepin_radon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we picked a **Normal distribution**, which is described by its two paramaters $\\mu$ (mean) and $\\sigma$ (standard deviation):\n",
    "\n",
    "$$y_i \\sim N(\\mu, \\sigma^2)$$\n",
    "\n",
    "Note that **Normal** and **Gaussian** is the same thing because Gaussian distributions are the most *normal*!\n",
    "\n",
    "This implies that we have 2 unknowns in the model; the **mean** and **standard deviation** of the distribution.\n",
    "\n",
    "While there may likely be prior information about the distribution of radon values, we will assume ***no prior knowledge*** (about the priors), and specify a **diffuse** prior for each parameter.\n",
    "\n",
    "Since the mean can take any real value (since it is on the log scale), we will use ***another normal distribution*** here, and specify a *large* variance to allow the possibility of very large or very small values:\n",
    "\n",
    "$$\\mu \\sim N(0, 10^2)$$\n",
    "\n",
    "For the standard deviation, we know that the true value must be positive (no negative variances!). Let's choose a [**uniform**](http://mathworld.wolfram.com/UniformDistribution.html) pdf prior, bounded from below at zero and from above at a value that is sure to be higher than any plausible value the true standard deviation (on the log scale) could take. That way, we are sure to capture the real value, somewhere in -between.\n",
    "\n",
    "$$\\sigma \\sim U(0, 10)$$\n",
    "\n",
    "We name our model and define our priors as the pdf's we mentionned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, Uniform\n",
    "\n",
    "with Model() as radon_model:\n",
    "    μ = Normal('μ', mu=0, sd=10)\n",
    "    σ = Uniform('σ', 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that remains is to add the **likelihood**, which takes $\\mu$ and $\\sigma$ as parameters, and the log-radon values as the set of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with radon_model:    \n",
    "    y = Normal('y', mu=μ, sd=σ, observed=hennepin_radon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will fit the model using a numerical approach called [**variational inference**](https://www.cs.jhu.edu/~jason/tutorials/variational.html), which is the default algorithm for PyMC3 (it is a more modern version of the Metropolis algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='10000' class='' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [10000/10000 02:30&lt;00:00 Average Loss = 134.7]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished [100%]: Average Loss = 134.68\n"
     ]
    }
   ],
   "source": [
    "from pymc3 import fit\n",
    "\n",
    "with radon_model:\n",
    "\n",
    "    samples = fit().sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to plot our posterior, which is the best possible values for $\\mu$ and $\\sigma$ that PyMC3 evaluated for us. In other words, the values of $\\mu$ and $\\sigma$ that result in the best possible $N(\\mu, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\arviz\\data\\io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pymc3 import plot_posterior\n",
    "\n",
    "plot_posterior(samples, var_names=['μ'], ref_val=np.log(4), color='LightSeaGreen');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the posterior distribution of $\\mu$, along with an estimate of the 95% posterior **credible interval**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\arviz\\data\\io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_posterior(samples, var_names=['σ'], ref_val=0.8, color='LightSeaGreen');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "What is the probability that a given household has a log-radon measurement larger than one? To answer this, we make use of the **posterior predictive distribution**, in its continuous formulation:\n",
    "\n",
    "$$p(z \\;| \\; y) = \\int_{\\theta} p(z \\;| \\; \\theta) p(\\theta \\; | \\; y) d\\theta$$\n",
    "\n",
    "where here $z$ is the predicted value and y is the data used to fit the model.\n",
    "\n",
    "The posterior predictive distribution accounts for uncertainty about $\\theta$. We should refrain from plugging in a single best estimate $\\hat{\\theta}$ for $\\theta$, because it ignores uncertainty about $\\theta$, and because a source of uncertainty is ignored, the predicted distribution will be too narrow (extreme values of $\\tilde {x}$ will occur more often than the posterior distribution suggests).\n",
    "\n",
    "The posterior distribution of possible $\\theta$ values depends on $\\mathbf {X}$: $p(\\theta \\; | \\; \\mathbf {X} )$. And the posterior predictive distribution of $\\tilde {x}$ given $\\mathbf {X}$ is calculated by marginalizing the distribution of $\\tilde {x}$ given $\\theta$ over the posterior distribution of $\\theta$ given $ \\mathbf {X}$. That is the integral above. See [here](https://en.wikipedia.org/wiki/Posterior_predictive_distribution) for more explanation.\n",
    "\n",
    "We can estimate the probability that a given household has a log-radon measurement larger than one from the posterior samples of the parameters in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = samples['μ']\n",
    "sigmas = samples['σ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "radon_samples = Normal.dist(mus, sigmas).random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.469"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(radon_samples > np.log(4)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior probability that a randomly-selected household in Henneprin County contains radon levels in excess of 4 pCi/L is 0.44.\n",
    "\n",
    "So that's good to know. But we did not do any **regressions** in the sense that we did not estimate our dependent variable as a function of an independent variable.\n",
    "\n",
    "So, let's do a regression now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radon regression\n",
    "\n",
    "We have multiple radon measurements (log-converted to be on the real line) in a county, and also whether the measurement has been taken in the basement (floor == 0) or on the first floor (floor == 1). \n",
    "\n",
    "Let's test the prediction that radon concentrations are higher in the basement by building a **linear model** where the radon concentration is a linear function of the floor of the house.\n",
    "\n",
    "We know that radon, a dangerous poisonous gas, seeps into a house from its foundation, so it makes sense that the basement has higher concentrations than the first floor, but let's *test this hypothesis*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: Pooling all measurements\n",
    "Let's pool all our data and estimate one linear regression to asses the influence of measurement across all counties:\n",
    "\n",
    "$$\\text{radon}_{i,c} = α + β∗\\text{floor}_{i,c} + ϵ$$\n",
    "\n",
    "Where $i$ represents the measurement, $c$ the county and $\\text{floor}$ contains which floor the measurement was made. \n",
    "\n",
    "Critically, we are only estimating ***one*** intercept and ***one*** slope for all measurements over all counties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: Separate statistics per county\n",
    "But what if we are interested whether different counties actually have different relationships (slope) and different base-rates of radon (intercept)? Then we have\n",
    "\n",
    "$$\\text{radon}_{i,c} = α_c + β_c∗\\text{floor}_{i,c} + ϵ_c$$\n",
    "\n",
    "Note that we added the index $c$ on the slopes and intercepts so we are estimating $n$ different $α$s and $β$s, one for each county.\n",
    "\n",
    ">**Note**: We also added a $c$ index on the error, allowing us to handle heteroscedastic data (data in which the variance is not constant across counties), but we could also handle heteroscedasticity within counties by assuming that the\n",
    "error is some function ϵ(µ) of the mean µ, or more accurately the conditional mean µ|X=x.\n",
    "\n",
    "Let's estimate this model. For each county a new estimate of the parameters is initiated. As we have no prior information on what the intercept or regressions could be, we are going to place a Normal distribution centered around 0 with a *wide* standard-deviation. We'll assume the measurements are **normally distributed** with noise $ϵ$ on which we place a Half-Cauchy distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_traces = {}\n",
    "for county_name in county_names:\n",
    "    # Select subset of data belonging to county\n",
    "    c_data = data.loc[data.county == county_name]\n",
    "    c_data = c_data.reset_index(drop=True)\n",
    "    \n",
    "    c_log_radon = c_data.log_radon\n",
    "    c_floor_measure = c_data.floor.values\n",
    "    \n",
    "    with pm.Model() as individual_model:\n",
    "        # Intercept prior\n",
    "        a = pm.Normal('alpha', mu=0, sigma=1)\n",
    "        \n",
    "        # Slope prior\n",
    "        b = pm.Normal('beta', mu=0, sigma=1)\n",
    "    \n",
    "        # Model error prior\n",
    "        eps = pm.HalfCauchy('eps', beta=1)\n",
    "    \n",
    "        # Linear model\n",
    "        radon_est = a + b * c_floor_measure\n",
    "    \n",
    "        # Data likelihood\n",
    "        y_like = pm.Normal('y_like', mu=radon_est, sigma=eps, observed=c_log_radon)\n",
    "\n",
    "        # Inference button (TM)!\n",
    "        trace = pm.sample(progressbar=True, cores=1)\n",
    "        \n",
    "    indiv_traces[county_name] = trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical regression\n",
    "There is a middle ground to both of these extreme views. Specifically, we may assume that while $α$s and $β$s are *different* for each county, the coefficients all come ***from a common group distribution***:\n",
    "\n",
    "$$α_c ∼ N(μ_α, σ^2_α)\\\\\n",
    "β_c ∼ N(μ_β, σ^2_β)$$\n",
    "\n",
    "This makes sense from a physics perspective because after all, all counties are in Minnesota and all radon gas deposits in Minnesota probably follow similar statistics.\n",
    "\n",
    "We thus assume the intercepts $α$ and slopes $β$ to originate from a normal distribution centered around their respective **group mean** $μ$ with a certain standard deviation $σ^2$, the values (or rather posteriors) of which we also estimate. \n",
    "\n",
    "That's why this is called **multilevel** or **hierarchical** modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hierarchical_model:\n",
    "    # Hyperpriors\n",
    "    mu_a = pm.Normal('mu_alpha', mu=0., sigma=1)\n",
    "    sigma_a = pm.HalfCauchy('sigma_alpha', beta=1)\n",
    "    mu_b = pm.Normal('mu_beta', mu=0., sigma=1)\n",
    "    sigma_b = pm.HalfCauchy('sigma_beta', beta=1)\n",
    "    \n",
    "    # Intercept for each county, distributed around group mean mu_a\n",
    "    a = pm.Normal('alpha', mu=mu_a, sigma=sigma_a, shape=len(data.county.unique()))\n",
    "    \n",
    "    # Intercept for each county, distributed around group mean mu_a\n",
    "    b = pm.Normal('beta', mu=mu_b, sigma=sigma_b, shape=len(data.county.unique()))\n",
    "    \n",
    "    # Model error\n",
    "    eps = pm.HalfCauchy('eps', beta=1)\n",
    "    \n",
    "    # Expected value\n",
    "    radon_est = a[county_idx] + b[county_idx] * data.floor.values\n",
    "    \n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal('y_like', mu=radon_est, sigma=eps, observed=data.log_radon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hierarchical_model:\n",
    "    hierarchical_trace = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(hierarchical_trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginal posteriors in the left column are highly informative. `mu_alpha` tells us the group mean (log) radon levels. `mu_beta` tells us that the slope is significantly negative (no mass above zero), meaning that radon concentrations are higher in the basement than first floor. We can also see by looking at the marginals for `alpha` that there is quite some differences in radon levels between counties. The different widths are related to how much measurements we have per county, the more, the higher our confidence in that parameter estimate.\n",
    "\n",
    "Let's plot some examples of county's showing the true radon values, the hierarchial predictions and the non-hierarchical predictions.\n",
    "\n",
    "The data points will be in black for three selected counties: CASS, CROW WING, and FREEBORN. The thick blue lines represent the mean estimate of the regression line of the individual model and the thick green lines represent the mean estimate of the regression line of the hierarchical model. \n",
    "\n",
    "The thinner lines are regression lines of individual samples from the posterior and give us a sense of how variable the estimates are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = ['CASS', 'CROW WING', 'FREEBORN']\n",
    "fig, axis = plt.subplots(1, 3, figsize=(12, 6), sharey=True, sharex=True)\n",
    "axis = axis.ravel()\n",
    "for i, c in enumerate(selection):\n",
    "    c_data = data.loc[data.county == c]\n",
    "    c_data = c_data.reset_index(drop = True)\n",
    "    z = list(c_data['county_code'])[0]\n",
    "\n",
    "    xvals = np.linspace(-0.2, 1.2)\n",
    "    for a_val, b_val in zip(indiv_traces[c]['alpha'][::10], indiv_traces[c]['beta'][::10]):\n",
    "        axis[i].plot(xvals, a_val + b_val * xvals, 'b', alpha=.05)\n",
    "    axis[i].plot(xvals, indiv_traces[c]['alpha'][::10].mean() + indiv_traces[c]['beta'][::10].mean() * xvals, \n",
    "                 'b', alpha=1, lw=2., label='individual')\n",
    "    for a_val, b_val in zip(hierarchical_trace['alpha'][::10][z], hierarchical_trace['beta'][::10][z]):\n",
    "        axis[i].plot(xvals, a_val + b_val * xvals, 'g', alpha=.05)\n",
    "    axis[i].plot(xvals, hierarchical_trace['alpha'][::10][z].mean() + hierarchical_trace['beta'][::10][z].mean() * xvals, \n",
    "                 'g', alpha=1, lw=2., label='hierarchical')\n",
    "    axis[i].scatter(c_data.floor + np.random.randn(len(c_data))*0.01, c_data.log_radon, \n",
    "                    alpha=1, color='k', marker='.', s=80, label='original data')\n",
    "    axis[i].set_xticks([0,1])\n",
    "    axis[i].set_xticklabels(['basement', 'first floor'])\n",
    "    axis[i].set_ylim(-1, 4)\n",
    "    axis[i].set_title(c)\n",
    "    if not i%3:\n",
    "        axis[i].legend()\n",
    "        axis[i].set_ylabel('log radon level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the county CASS we see that the non-hierarchical estimation has huge uncertainty about the radon levels of first floor measurements -- that's because we don't have any measurements in this county. The hierarchical model, however, is able to apply what it learned about the relationship between floor and radon-levels from *other counties* to CASS and make sensible predictions even in the absence of measurements.\n",
    "\n",
    "We can also see how the hierarchical model produces more robust estimates in CROW WING and FREEBORN. In this regime of few data points the non-hierarchical model reacts more strongly to individual data points because that's all it has to go on.\n",
    "\n",
    "Having the group-distribution constrain the coefficients we get meaningful estimates in all cases as we apply what we learn from the group to the individuals and vice-versa.\n",
    "\n",
    "## Conclusion\n",
    "A multi-level hierarchical Bayesian model gives the best of both worlds when we have multiple sets of measurements we expect to have similarity. \n",
    "\n",
    "The naive approach either pools all data together and ignores the individual differences, or treats each set as completely separate leading to noisy estimates as shown above. \n",
    "\n",
    "By placing a group distribution on the individual sets we can learn about each set and the group simultaneously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Verifying the model with out-of-sample predictions\n",
    "Let's see how we can verify a GLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(style=\"darkgrid\", rc={\"axes.facecolor\": \".9\", \"grid.color\": \".8\"})\n",
    "sns.set_palette(palette=\"deep\")\n",
    "sns_c = sns.color_palette(palette=\"deep\")\n",
    "\n",
    "import arviz as az\n",
    "import patsy\n",
    "import pymc3 as pm\n",
    "\n",
    "from pymc3 import glm\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [7, 6]\n",
    "plt.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Dataset \\#1\n",
    "Bayesian linear regression is a more informative version of standard linear regression. Linear regression gives you *point values* for model parameters as well as predictions. Bayesian linear regression, in turn, gives you *probability distributions*.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/linear-regr-vs-bayesian.webp\" width=600 />\n",
    "</left>\n",
    "\n",
    "Let's generate a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "   -1.64934805,  0.52925273,  1.10100092,  0.38566793, -1.56768245,\n",
    "    1.26195686,  0.92613986, -0.23942803,  0.33933045,  1.14390657,\n",
    "    0.65466195, -1.36229805, -0.32393554, -0.23258941,  0.17688024,\n",
    "    1.60774334, -0.22801156,  1.53008133, -1.31431042, -0.27699609\n",
    "] # inputs\n",
    "y = [\n",
    "   -3.67385666,  3.37543275,  6.25390538,  1.41569973, -2.08413872,\n",
    "    6.71560158,  6.32344159,  2.40651236,  4.54217349,  6.25778739,\n",
    "    4.98933806, -2.69713137,  1.45705571, -0.49772953,  1.50502898,\n",
    "    7.27228263,  1.6267433 ,  6.43580518, -0.50291509,  0.65674682\n",
    "] # outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical statistics\n",
    "To build a regression model with sklearn's `LinearRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(x).reshape(-1, 1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(reg.score(X, y))\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the model is $y = 3.2x + 2.19$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of values in the best fit line\n",
    "abline_values = [reg.coef_ * i + reg.intercept_ for i in np.linspace(-1.5, 1.5, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "#ax.axline((0, reg.intercept_), slope=reg.coef_, color='C0', label='by slope')\n",
    "plt.plot(np.linspace(-1.5, 1.5, 10), abline_values, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply statistical hypothesis testing and check the p-value for the slope and determine if this apparent positive connection between x and y is significant. The t-statistic is around 13.8, the p-value is around 0.0 and a 95% confidence interval is [2.7, 3.7]. In short, this means that x has a significant positive influence on y.\n",
    "\n",
    "The standard linear regression model outputs a single y value for each input x. Do you want to know the prediction for x = 0? Well, it is y = 2.19, take it or leave it.\n",
    "\n",
    "There are ways to address this data insecurity not only for linear regression but for any regression algorithm via bootstrapping. In short: Subsample your data, fit a model on a subsample and use it to make a prediction. Repeat this 1000 times and you obtain 1000 different predictions.\n",
    "\n",
    "You can compute the mean $μ$, the standard deviation $σ$, quantiles, or anything you want of these 1000 samples. If you still want to output a single number, just take the mean. However, now you also have the choice of outputting intervals, such as $[μ-3σ, μ+3σ]$.\n",
    "\n",
    "This approach might work well, but it’s extremely computationally expensive since we have to fit a model 1000 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian statistics\n",
    "In Bayesian statistics, we deal with distribution. This makes it necessary to specify linear regression as a distribution as well. One assumption commonly used for the standard linear regression approach is the following: $y$ is normally distributed with mean $ax+b$ (the quantity we always predict) and some standard deviation $σ$:\n",
    "\n",
    "$$y \\approx N(ax+b, \\sigma)$$\n",
    "\n",
    "in other words, our data likelihood is a gaussian distribution with parameters $\\mu$ (or, equivalently, as $a$ and $b$) and $\\sigma$, where $\\mu$ is modelled as an additive linear model.\n",
    "\n",
    "In Bayesian statistics, we don’t treat ($\\mu$, or) $a$ and $b$ as fixed variables, but also as coming from distributions.\n",
    "\n",
    "If we say $a, b \\approx N(0, 16)$, we tell the model that without observingany data, we expect the slope and intercept of the line to be around zero with a standard deviation of four, i.e. we expect a and b to be around -12 and 12. \n",
    "\n",
    "The prior is something we have to play around with if things don’t work out. This means that PyMC3 does not give a proper result, or the prediction performance of the model is bad.\n",
    "\n",
    "How about $\\sigma$? We could give it the same normal distributions as $a$ and $b$. What we do instead is model $σ²$ directly because we never need $σ$ directly, but only $σ²$ for the variance of $y$. Since $σ²$ is a positive number, we choose a prior distribution that yields exclusively positive values as well, such as the Exponential distribution, the Gamma distribution, or the Half-normal distribution.\n",
    "\n",
    "The density of the exponential distribution is always decreasing. Using this distribution as a prior tells the model that we expect a small, but also positive value for $σ²$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # define priors\n",
    "    a = pm.Normal('slope', 0, 16)\n",
    "    b = pm.Normal('intercept', 0, 16)\n",
    "    s = pm.Exponential('error', 1)\n",
    "    \n",
    "    # predictions\n",
    "    obs = pm.Normal('observation', a*x + b, s, observed=y)\n",
    "    \n",
    "    # use MCMC to sample\n",
    "    trace = pm.sample()\n",
    "    \n",
    "az.plot_posterior(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model comes to the same conclusion as the standard linear regression: A slope of around 3.2 and an intercept of around 2.2. We even see that the standard error is about 1.1. But here, we also get credible intervals for free!\n",
    "\n",
    "Looking at these figures, it seems like the model is *still unsure about the parameters*: Look at the slope, for example. With a probability of 94%, the slope $a$ is between 2.7 and 3.7, which is still a rather wide interval. The same goes for the other parameters we estimated.\n",
    "\n",
    ">The great thing about Bayesian reasoning: All of these distributions narrow down further, i.e. the model gets more certain the more data we provide\n",
    "\n",
    "### Generating predictions\n",
    "What we need to do is to generate a variable that is a container and can be changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as predictive_model:\n",
    "    a = pm.Normal('slope', 0, 16)\n",
    "    b = pm.Normal('intercept', 0, 16)\n",
    "    s = pm.Exponential('error', 1)\n",
    "    \n",
    "    x_ = pm.Data('features', x) # a data container, can be changed\n",
    "    \n",
    "    obs = pm.Normal('observation', a*x_ + b, s, observed=y)\n",
    "    \n",
    "    trace = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We basically told the model to use a placeholder x_ which was initially filled with our training data x. \n",
    "\n",
    "We then train the model, i.e. get posterior distributions for all of the parameters. \n",
    "\n",
    "We can now pass the model new data via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.linspace(-3, 3, 50) # 50 input values between -3 and 3\n",
    "\n",
    "with predictive_model:\n",
    "    pm.set_data({'features': x_new})\n",
    "    posterior = pm.sample_posterior_predictive(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`posterior` is a dictionary that contains the new observations. Let’s grab them and store them as `y_pred`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = posterior['observation']\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable y_pred is a numpy array containing 2000 observations for each of the 50 inputs in x_new , hence its dimensions are 2000 x 50. \n",
    "\n",
    "For example, `y_pred[:, 0]` gives us 50 predictions, one for each entry in `x_new`, while `y_pred[0, :]` gives us 4000 predictions for the first element in `x_new`, namely -3.\n",
    "\n",
    "Using this table, we can get the mean and the standard deviation across the 4000 samples for each of the 50 new inputs and visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = y_pred.mean(axis=0)\n",
    "y_std = y_pred.std(axis=0)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.scatter(x, y, c='k', zorder=10, label='Data')\n",
    "plt.plot(x_new, y_mean, label='Prediction Mean')\n",
    "plt.fill_between(x_new, y_mean - 3*y_std, y_mean + 3*y_std, alpha=0.33, label='Uncertainty Interval ($\\mu\\pm3\\sigma$)')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.ylim(-14, 16)\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the GLM Submodule, we rewrite our first simple model as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as simpler_model:\n",
    "    pm.glm.GLM.from_formula('y ~ x', dict(x=x, y=y))\n",
    "    \n",
    "    trace = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you want to change the default priors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "my_priors = {\n",
    "    'Intercept': pm.Cauchy.dist(0, 10),\n",
    "    'x': pm.Cauchy.dist(0, 10)\n",
    "}\n",
    "\n",
    "with pm.Model() as model:\n",
    "    pm.glm.GLM.from_formula('y ~ x', df, priors=my_priors)\n",
    "    \n",
    "    trace = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add as many variables as you wish as long as your machine can handle them:\n",
    "```\n",
    "a = pm.Normal('slopes', 0, 16, shape=3)\n",
    "```\n",
    "\n",
    "You can use different distributions than the normal distribution for y, for example, the Student’s t-distribution or the Cauchy distribution.\n",
    "\n",
    "The tails of these two distributions are wider than the ones of the normal distribution. This has the effect that outliers don’t surprise, and hence influence the model as much as in the case of the normal distribution.\n",
    "\n",
    "If you don’t want to predict continuous values at all but counts instead. In this case, use the Poisson, binomial, or negative binomial distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    pm.glm.GLM.from_formula('y~x', df, family=pm.families.Poisson())\n",
    "    \n",
    "    trace = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Dataset \\#2\n",
    "For a more complex model, we will fit a **logistic regression model** where there is principally a multiplicative interaction between two numerical features. In fact, this model blows up on my laptop with the classical `Bad Initial Energy` error that underscores a bad fit. But I am curious if it works on yours!\n",
    "\n",
    "We will use GLM's [patsy](https://patsy.readthedocs.io/en/latest/) formulas to make things simpler.\n",
    "\n",
    "First, let's generate our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 47\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Number of data points\n",
    "n = 250\n",
    "\n",
    "# Create features\n",
    "x1 = np.random.normal(loc=0.0, scale=2.0, size=n)\n",
    "x2 = np.random.normal(loc=0.0, scale=2.0, size=n)\n",
    "epsilon = np.random.normal(loc=0.0, scale=0.5, size=n)\n",
    "\n",
    "# Define target variable\n",
    "intercept = -0.5\n",
    "beta_x1 = 1\n",
    "beta_x2 = -1\n",
    "beta_interaction = 2\n",
    "z = intercept + beta_x1 * x1 + beta_x2 * x2 + beta_interaction * x1 * x2\n",
    "p = 1 / (1 + np.exp(-z))\n",
    "y = np.random.binomial(n=1, p=p, size=n)\n",
    "\n",
    "# Build our dataframe\n",
    "df = pd.DataFrame(dict(x1=x1, x2=x2, y=y))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(style=\"darkgrid\", rc={\"axes.facecolor\": \".9\", \"grid.color\": \".8\"})\n",
    "sns.set_palette(palette=\"deep\")\n",
    "sns_c = sns.color_palette(palette=\"deep\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [7, 6]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "sns.pairplot(\n",
    "    data=df, kind=\"scatter\", height=2, plot_kws={\"color\": sns_c[1]}, diag_kws={\"color\": sns_c[2]}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that:\n",
    "\n",
    "- $x_1$ and $x_2$ are not correlated.\n",
    "\n",
    "- $x_1$ and $x_2$ do not seem to separate the $y$-classes independently.\n",
    "\n",
    "- The distribution of $y$ is not highly unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns_c_div = sns.diverging_palette(240, 10, n=2)\n",
    "sns.scatterplot(x=\"x1\", y=\"x2\", data=df, hue=\"y\", palette=[sns_c_div[0], sns_c_div[-1]])\n",
    "ax.legend(title=\"y\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "ax.set(title=\"Sample Data\", xlim=(-9, 9), ylim=(-9, 9));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create features from a formula using [patsy](https://patsy.readthedocs.io/en/latest/) (also [here](https://learn-scikit.oneoffcoder.com/patsy.html)) directly and then use class `pymc3.glm.linear.GLM`. \n",
    "\n",
    "Let's start with a formula based on just the interaction effect between $x_1$ and $x_2$, since it's the strongest effect in the data we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "\n",
    "# Define model formula\n",
    "formula = \"y ~ x1 * x2\"\n",
    "\n",
    "# Create features\n",
    "y, x = patsy.dmatrices(formula_like=formula, data=df)\n",
    "y = np.asarray(y).flatten()\n",
    "labels = x.design_info.column_names\n",
    "x = np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a train-test split in order to safegaurd part of the data for the purpose of testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define and fit the model and, why not, modify the default priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import pymc3 as pm\n",
    "from pymc3 import glm\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Set data container\n",
    "    data = pm.Data(\"data\", x_train)\n",
    "    \n",
    "    # Define GLM family\n",
    "    family = pm.glm.families.Binomial()\n",
    "    \n",
    "    # Set user-defined priors\n",
    "    priors = {\n",
    "        \"Intercept\": pm.Normal.dist(mu=0, sd=10),\n",
    "        \"x1\": pm.Normal.dist(mu=0, sd=10),\n",
    "        \"x2\": pm.Normal.dist(mu=0, sd=10),\n",
    "        \"x1:x2\": pm.Normal.dist(mu=0, sd=10),\n",
    "    }\n",
    "    \n",
    "    # Specify model\n",
    "    glm.GLM(y=y_train, x=data, family=family, intercept=False, labels=labels, priors=priors)\n",
    "    \n",
    "    # Configure and run sampler\n",
    "    trace = pm.sample(5000, chains=5, tune=1000, target_accept=0.87, random_seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot chains\n",
    "az.plot_trace(data=trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate predictions (what are called **out-of-sample** predictions or [posterior predictive checks](https://pymc3-testing.readthedocs.io/en/rtd-docs/notebooks/posterior_predictive.html)) in order to verify our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data reference\n",
    "pm.set_data({\"data\": x_test}, model=model)\n",
    "\n",
    "# Generate posterior samples\n",
    "ppc_test = pm.sample_posterior_predictive(trace, model=model, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the point prediction by taking the mean and defining the category via a threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test_pred = ppc_test[\"y\"].mean(axis=0)\n",
    "y_test_pred = (p_test_pred >= 0.5).astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute accuracy on the test set since we have the label for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"accuracy = {accuracy_score(y_true=y_test, y_pred=y_test_pred): 0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85% accuracy. Not bad.\n",
    "\n",
    "We can also plot the [Receiver operating characteristic curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (ROC) and the [Area Under the Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) (AUC).\n",
    "\n",
    "A ROC curve, first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
    "\n",
    "The **true-positive** (TP) rate is also known as sensitivity, recall or probability of detection.\n",
    "\n",
    "The **false-positive** (FP) rate is also known as probability of false alarm.\n",
    "\n",
    "In statistical hypothesis testing, a [type I error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error) is the mistaken rejection of an actually true null hypothesis (also known as a **false positive** finding or conclusion; example: *an innocent person is convicted*), while a [type II error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error) is the failure to reject a null hypothesis that is actually false (also known as a **false negative**, or **oopsie** finding or conclusion; example: *a guilty person is not convicted*).\n",
    "\n",
    "In a two-class prediction problem (binary classification), the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier: If the outcome from a prediction is p and the actual value is also p, then it is called a **true positive** (TP); however if the actual value is n then it is said to be a **false positive** (FP or *false alarm*). Conversely, a true negative (TN) has occurred when both the prediction outcome and the actual value are n, and false negative (FN or *oopsie*) is when the prediction outcome is n while the actual value is p.\n",
    "\n",
    "To draw a ROC curve, only the true positive rate (TPR) and false positive rate (FPR) are needed (as functions of some classifier parameter). The TPR defines how many correct positive results occur among all positive samples available during the test. FPR, on the other hand, defines how many incorrect positive results occur among all negative samples available during the test.\n",
    "\n",
    "The best possible prediction method would yield a point in the upper left corner or coordinate (0,1) of the ROC space, representing 100% sensitivity (100% true positives or no false negatives) and 100% specificity (0 false positives or 100% true negatives). The (0,1) point is also called a perfect classification. \n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/roc-curve.png\" width=400 />\n",
    "</left>\n",
    "\n",
    "A random guess would give a point along a diagonal line (the so-called line of no-discrimination) from the bottom left to the top right corners (regardless of the positive and negative base rates). An intuitive example of random guessing is a decision by flipping coins. As the size of the sample increases, a random classifier's ROC point tends towards the diagonal line. In the case of a balanced coin, it will tend to the point (0.5, 0.5).\n",
    "\n",
    "The machine learning community most often uses the ROC AUC statistic for model comparison. ROC AUC varies between 0 and 1 — with an uninformative classifier yielding 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, auc, roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(\n",
    "    y_true=y_test, y_score=p_test_pred, pos_label=1, drop_intermediate=False\n",
    ")\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "roc_display = roc_display.plot(ax=ax, marker=\"o\", color=sns_c[4], markersize=4)\n",
    "ax.set(title=\"ROC\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is performing as expected (of course know the data generating process, which is almost never the case in practical applications).\n",
    "\n",
    "Finally we describe and plot the model decision boundary, which is the space defined as\n",
    "\n",
    "$$\\mathcal{B} = \\{(x_1, x_2) \\in \\mathbb{R}^2 \\: | \\: p(x_1, x_2) = 0.5\\}$$\n",
    "\n",
    "where $p$ denotes the probability of belonging to the class $y=1$ output by the model. To make this set explicit, we simply write the condition in terms of the model parametrization\n",
    "\n",
    "$$0.5 = \\frac{1}{1 + \\exp(-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1x_2))}$$\n",
    "\n",
    "which implies\n",
    "\n",
    "$$0 = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1x_2$$\n",
    "\n",
    "Solving for $x_2$ we get the formula\n",
    "\n",
    "$$x_2 = - \\frac{\\beta_0 + \\beta_1 x_1}{\\beta_2 + \\beta_{12}x_1}$$\n",
    " \n",
    "Observe that this curve is a hyperbola centered at the singularity point $x_1 = - \\beta_2 / \\beta_{12}$.\n",
    "\n",
    "Let's plot the model decision boundary using posterior predictions on a grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct grid.\n",
    "x1_grid = np.linspace(start=-9, stop=9, num=300)\n",
    "x2_grid = x1_grid\n",
    "\n",
    "x1_mesh, x2_mesh = np.meshgrid(x1_grid, x2_grid)\n",
    "\n",
    "x_grid = np.stack(arrays=[x1_mesh.flatten(), x2_mesh.flatten()], axis=1)\n",
    "\n",
    "# Create features on the grid.\n",
    "x_grid_ext = patsy.dmatrix(formula_like=\"x1 * x2\", data=dict(x1=x_grid[:, 0], x2=x_grid[:, 1]))\n",
    "\n",
    "x_grid_ext = np.asarray(x_grid_ext)\n",
    "\n",
    "# Generate model predictions on the grid.\n",
    "pm.set_data({\"data\": x_grid_ext}, model=model)\n",
    "ppc_grid = pm.sample_posterior_predictive(trace, model=model, samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the model decision boundary on the grid for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = -(trace[\"Intercept\"].mean(axis=0) + trace[\"x1\"].mean(axis=0) * x1_grid)\n",
    "denominator = trace[\"x2\"].mean(axis=0) + trace[\"x1:x2\"].mean(axis=0) * x1_grid\n",
    "bd_grid = numerator / denominator\n",
    "\n",
    "grid_df = pd.DataFrame(x_grid, columns=[\"x1\", \"x2\"])\n",
    "grid_df[\"p\"] = ppc_grid[\"y\"].mean(axis=0)\n",
    "grid_df.sort_values(\"p\", inplace=True)\n",
    "\n",
    "p_grid = grid_df.pivot(index=\"x2\", columns=\"x1\", values=\"p\").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the decision boundary as well as predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "cmap = sns.diverging_palette(240, 10, n=50, as_cmap=True)\n",
    "sns.scatterplot(\n",
    "    x=x_test[:, 1].flatten(),\n",
    "    y=x_test[:, 2].flatten(),\n",
    "    hue=y_test,\n",
    "    palette=[sns_c_div[0], sns_c_div[-1]],\n",
    "    ax=ax,\n",
    ")\n",
    "sns.lineplot(x=x1_grid, y=bd_grid, color=\"black\", ax=ax)\n",
    "ax.contourf(x1_grid, x2_grid, p_grid, cmap=cmap, alpha=0.3)\n",
    "ax.legend(title=\"y\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "ax.lines[0].set_linestyle(\"dotted\")\n",
    "ax.set(title=\"Model Decision Boundary\", xlim=(-9, 9), ylim=(-9, 9), xlabel=\"x1\", ylabel=\"x2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've computed the model decision boundary by using the mean of the posterior samples. \n",
    "\n",
    "However, we can generate a *better* (and more informative) plot if we use the *complete* distribution (similarly for other metrics like accuracy and auc). One way of doing this is by storing and computing it inside the model definition as a pymc3 `Deterministic` variable.\n",
    "\n",
    "### Conclusion\n",
    "We only modelled the multiplicative interaction between independent variables $x_1$ and $x_2$. In other words, we neglected the intercept and the additive $x_1$ and $x_2$ effects. A more correct model would be to use this patsy formula: `y ~ x1 + x2 + x1:x2`.\n",
    "\n",
    "Another way to verify the model (and what we've mostly used in class) is to compute empirical statistical moments and model statistical moments, and see how equal they are.\n",
    "\n",
    "Models are very easy to build when the data was simulated to have exactly the kind of statistical relationship we atttempt to build. This example hides the main difficulty in data science: Identifying the data likelihood function.\n",
    "\n",
    "To underscore this, I run the same Bayesian simulation using my NLP model attempt to link moral words in Mao's paragraphs with the number of friend and enemy keywords. To make the sim exactly the same, I partition paragraphs into ones with a lot of moral words (>7), and with less moral words (<7).\n",
    "\n",
    "We will see how pymc3 fails to build the model with the classic `Bad Initial Energy` error. I still believe that ***if a model is possible***, the negative binomial is the better data likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for root, dirs, files in os.walk(\"c:/Users/Dino/mao/resolved\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "             print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mao_paragraphs = []\n",
    "for root, dirs, files in os.walk(\"c:/Users/Dino/mao/resolved\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            print(os.path.join(root, file))\n",
    "            with open(os.path.join(root, file), \"r\") as input:\n",
    "                paragraphs = input.read().split(\"\\n\\n\")   #\\n\\n denotes there is a blank line in between paragraphs.\n",
    "            #print(paragraphs[0])\n",
    "            mao_paragraphs.extend(paragraphs)\n",
    "            \n",
    "print(len(mao_paragraphs))\n",
    "print(mao_paragraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_any_words_paragraphs(paragraphs, words):\n",
    "    \n",
    "    word_para = []\n",
    "    for p in tqdm(paragraphs):\n",
    "        total = 0\n",
    "        for w in words:\n",
    "            total += p.count(w)\n",
    "        word_para.append(total)\n",
    "            \n",
    "    return word_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_words = []\n",
    "with open('D:/user/docs/NU/_Info6105/fp/fa.22/mfd.txt', \"r\") as input:\n",
    "    pair_lines = input.read().split(\"\\n\")\n",
    "for p in pair_lines:\n",
    "    moral_words.append(p.split('\\t')[0])\n",
    "print(moral_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_occurences = get_any_words_paragraphs(mao_paragraphs, moral_words)\n",
    "plt.plot(moral_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enemy_words = ['tyrants', 'evil', 'landlord', 'gentry', 'enemy', 'bourgeoisie']\n",
    "#friend_words = ['peasant', 'red army', 'party', 'people', 'association', 'masses', 'poor', 'committee', 'comrades', 'workers']\n",
    "enemy_words = ['tyrants', 'landlord', 'gentry', 'bourgeoisie']\n",
    "friend_words = ['peasant', 'masses', 'poor', 'workers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friend_occurences = get_any_words_paragraphs(mao_paragraphs, friend_words)\n",
    "enemy_occurences = get_any_words_paragraphs(mao_paragraphs, enemy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(friend_occurences)\n",
    "plt.plot(enemy_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moral = pd.DataFrame(list(zip(moral_occurences, friend_occurences, enemy_occurences)), \n",
    "                        columns =['morals', 'friend', 'enemy'])\n",
    "df_moral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    data=df_moral, kind=\"scatter\", height=2, plot_kws={\"color\": sns_c[1]}, diag_kws={\"color\": sns_c[2]}\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_bernoulli = [1.0 if v > 7 else 0.0 for v in df_moral.morals.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moral2 = pd.DataFrame(list(zip(moral_bernoulli, friend_occurences, enemy_occurences)), \n",
    "                        columns =['moral_p', 'friend', 'enemy'])\n",
    "df_moral2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    data=df_moral2, kind=\"scatter\", height=2, plot_kws={\"color\": sns_c[1]}, diag_kws={\"color\": sns_c[2]}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected from our time series plots, enemy and friend words are not that uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns_c_div = sns.diverging_palette(240, 10, n=2)\n",
    "sns.scatterplot(x=\"friend\", y=\"enemy\", data=df_moral2, hue=\"moral_p\", palette=[sns_c_div[0], sns_c_div[-1]])\n",
    "ax.legend(title=\"y\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "ax.set(title=\"Moral paragraphs as a function of the presence friend and enemy words\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "\n",
    "# Define model formula\n",
    "formula = \"moral_p ~ friend + enemy + friend * enemy\"\n",
    "\n",
    "# Create features\n",
    "y, X = patsy.dmatrices(formula_like=formula, data=df_moral2, return_type='dataframe')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.asarray(X)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y.moral_p.values\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"labels = {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X2, y2, train_size=0.90, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_binom3:\n",
    "    # Set data container\n",
    "    data = pm.Data(\"data\", x_train)\n",
    "    \n",
    "    # Define GLM family\n",
    "    family = pm.glm.families.Binomial()\n",
    "    \n",
    "    # Set priors\n",
    "    priors = {\n",
    "        \"Intercept\": pm.Normal.dist(mu=0, sd=10),\n",
    "        \"x1\": pm.Normal.dist(mu=0, sd=10),\n",
    "        \"x2\": pm.Normal.dist(mu=0, sd=10),\n",
    "        \"x1:x2\": pm.Normal.dist(mu=0, sd=10),\n",
    "    }\n",
    "    \n",
    "    # Specify model\n",
    "    glm.GLM(y=y_train, x=data, family=family, intercept=False, labels=labels, priors=priors)\n",
    "    \n",
    "    # Configure sampler\n",
    "    trace = pm.sample(5000, chains=5, tune=1000, target_accept=0.77, random_seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusion\n",
    "**General Linear Regression models** is how we do data science when we think there is a **linear** relationship between one of the statistical moments of a dependent variable (most frequently the mean of the dependent variable), and independent variables. Adding non-linear effects is also just a matter of adding new (non-linear) variables (even though it may involve a lot of exploration to find the right variable: is it $x^2$, $x^3$, $log(x)$?).\n",
    "\n",
    "Pymc3 provides awesome boilerplate code to simplify the task so we don't have to write a lot of code, in Dua Lipa style!\n",
    "\n",
    "Here is a synopsis of things to remember about GLMs:\n",
    "\n",
    "- GLMs deftly side-step several strong requirements of classical linear models such as additiveness of effects, homoscedasticity of data and normality of residual errors.\n",
    "- GLMs impose a common functional form on all models in the GLM family which consists of a link function g(µ|X=x) that allows you to express the transformed conditional mean of the dependent variable y as a linear combination of the regression variables X.\n",
    "- GLMs require the specification of a suitable variance function V(µ|X=x) for expressing the conditional variance in the data as function of the condition mean. What form V(.) takes depends on the probability distribution that you assume for the dependent variable y in your data set.\n",
    "- GLMs do not care about the distributional form of the error term, thereby making them a practical choice for many real world data sets.\n",
    "- GLMs do assume that regression variables X are uncorrelated, thereby making GLMs unsuitable for modeling auto-correlated time series data.\n",
    "\n",
    "What's the difference between this kind of data science and classic ML like RFs and ANNs? This data science works when datasets are well-behaved, which means their histograms are shaped so that they can be approximated with analytic functions like the gaussian, the gamma, the beta, etc. In other words, when we can assign an analytic data likelihood to our dataset. When that's not possible, that's what we'll study next semester!\n",
    "\n",
    "# References\n",
    "There is a library that I know of but have not yet had the chance to experiment with. It's called [bambi](https://bambinos.github.io/bambi/main/index.html) that works with pymc3 and is designed to make it Dua Lipa easy to fit Bayesian mixed-effects models common in biology, social sciences and other disciplines. [Here's](https://bambinos.github.io/bambi/main/notebooks/negative_binomial.html) their negative binomial exemplar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
