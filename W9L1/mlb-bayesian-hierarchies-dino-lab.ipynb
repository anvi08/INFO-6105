{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 9 Day 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 31 October 2022, with material by by John K. Kruschke</div>\n",
    "\n",
    "# Bayesian Hierarchies\n",
    "Just like **objects** derive from other **objects** in OO programming, ***specialized*** models derive from more ***abstract*** models in Data Science. In this way, we can produce hierarchies that yield more abstract models from more specialized datasets. \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/phillies-astros.jfif\" width=600 />\n",
    "</center>\n",
    "\n",
    "Every summer, americans get together to Watch what they call the ***World Series*** of baseball.\n",
    "\n",
    "This summer, it's the Philadelphia Phillies versus the Houston Astros.\n",
    "\n",
    "We've modelled drivers and teams, but how do we model ***groups*** of teams or players?\n",
    "\n",
    "For example, all greek soccer teams versus all English soccer teams? I bet the english soccer teams come on top.\n",
    "\n",
    "We model that with Bayesian **hierarchies**.\n",
    "\n",
    "The hierarchical structure of a model is an expression of how you think the data should be meaningfully modeled and the model description captures *group* aspects of the data that you care about. Using Bayesian estimation, you supply a prior distribution on group parameters, and infer an entire posterior distributions across the joint parameter space.\n",
    "\n",
    "In World Series, pitchers get to bat as well. But pitchers are traine to *throw* a ball, not to *bat* it. Is that the same in cricket? I bet that pitchers have much worse batting averages than batters, but how to prive that with Bayesian simulations?\n",
    "\n",
    "## Batting avergages in Baseball\n",
    "During a year of games, different players have different numbers of opportunities at bat, and on some of these opportunities a player might actually hit the ball.\n",
    "\n",
    "That ratio, of hits divided by opportunities at bat, is called the [batting average](https://en.wikipedia.org/wiki/Batting_average_(baseball)) of each player.\n",
    "\n",
    "Assume we have data consisting of records from 948 players in the 2012 regular season of Major League Baseball.\n",
    "\n",
    "To give some sense of the data, there were 324 pitchers with a median of 4.0 at bats, 103 catchers with a median of 170.0 at - bats, and 60 right fielders with a median of 340.5 at bats, along with 461 players in six other positions.\n",
    "\n",
    "For every pair of players, we could ask how much their estimated batting abilities differ. \n",
    "\n",
    "For every pair of positions, we can also ask how much their batting abilities differ. For example, do outfielders have different batting averages than basemen?\n",
    "\n",
    "We need to estimate the batting abilities for individual players, for positions, and for groups of players. Clearly, we expect the  batting ability of pitchers to be lower than that for catchers, for example.\n",
    "\n",
    "Each category has its own modal bias $ω_c$, from which all subject biases in the category are assumed to be drawn.\n",
    "\n",
    "When $κ$ is large, the category biases $ω_c$ are tightly concentrated.\n",
    "\n",
    "A prior on $κ_c$ applies independently to each $κ_c$ in a manner fixed by the prior constants $\\alpha_κ$ and $beta_κ$, and the $κ_c$’s do not mutually inform each other via that part of the hierarchy.\n",
    "\n",
    "Each row could also contain a unique subject identifier.\n",
    "\n",
    "Let's start by putting all 948 players under a single over-arching distribution (the pooled model), then the estimates for two players with identical batting records would be identical regardless of the position they play.\n",
    "\n",
    "Players with many at bats should have somewhat less shrinkage of their individual estimates than players with few at-bats and who maybe had estimates dominated by position information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0) # to keep it reproducible\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>PriPos</th>\n",
       "      <th>Hits</th>\n",
       "      <th>AtBats</th>\n",
       "      <th>PlayerNumber</th>\n",
       "      <th>PriPosNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fernando Abad</td>\n",
       "      <td>Pitcher</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bobby Abreu</td>\n",
       "      <td>Left Field</td>\n",
       "      <td>53</td>\n",
       "      <td>219</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tony Abreu</td>\n",
       "      <td>2nd Base</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dustin Ackley</td>\n",
       "      <td>2nd Base</td>\n",
       "      <td>137</td>\n",
       "      <td>607</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Matt Adams</td>\n",
       "      <td>1st Base</td>\n",
       "      <td>21</td>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Player      PriPos  Hits  AtBats  PlayerNumber  PriPosNumber\n",
       "0  Fernando Abad     Pitcher     1       7             1             1\n",
       "1    Bobby Abreu  Left Field    53     219             2             7\n",
       "2     Tony Abreu    2nd Base    18      70             3             4\n",
       "3  Dustin Ackley    2nd Base   137     607             4             4\n",
       "4     Matt Adams    1st Base    21      86             5             3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(\n",
    "    map(pd.read_csv, ['data/BattingAverage.csv']), ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_players = len(df)\n",
    "num_players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain the intuition behind the parametric form of the [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) a bit more. Very important for baseball. \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/atbat.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "Whereas the Poisson distribution is a *count* statistic, the Beta distribution can be thought of as representing a distribution of probabilities- that is, it represents all the possible values of a probability when we don't know what that probability is. The domain of the Beta distribution is (0, 1), just like a probability, so we already know we're on the right track.\n",
    "\n",
    "Imagine you want to predict a baseball player's season-long batting average.  We know it's somewhere around .300, like we know the average number of text phones I can expect my girlfriend to receive per day before I get worried too much.\n",
    "\n",
    "The baseball player can get into a lucky streak and get an average of 1.000, or an unlucky streak with an average of 0, neither of which are a good predictor. Let's go in with prior expectations: In history, most season batting averages have hovered between something like .200 and .350.\n",
    "\n",
    "Let's plot a Beta distribution with parameters α=81 and β=219. We know that the formula for the mean of the beta distribution is:\n",
    "\n",
    "$$ \\frac{\\alpha}{\\alpha + \\beta} = \\frac{81}{81+219} =.270$$\n",
    "\n",
    "and the distribution lies almost entirely within the reasonable range for a batting average \\[.2, .35\\].\n",
    "\n",
    "Plot histogram:\n",
    "```\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = stats.beta.rvs(81, 219, loc = 0, scale = 1, size=1000)\n",
    "plt.hist(data,bins='auto',normed=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine now the player gets his first at bat and has a single hit. His record for the season is now 1 hit; 1 at bat. \n",
    "\n",
    "We update our probabilities by shifting the entire curve over just a bit to reflect our new information. The new Beta distribution will be:\n",
    "\n",
    "$$ \\text{Beta}(\\alpha_0 + \\text{hits}, \\beta_0 + \\text{misses})$$\n",
    " \n",
    "Where $α_0$ and $β_0$ are the parameters we started with: 81 and 219. Thus, in this case, $α$ has increased by 1 (his one hit), while $β$ has not increased at all (no misses yet). Read [here](https://en.wikipedia.org/wiki/Conjugate_prior#Example) for proof. It's about *conjugate priors*: The beta distribution is a conjugate prior for a binomial data likelihood. This means that if the data likelihood is a binomial and the prior is a beta, the posterior will also be a beta.\n",
    "\n",
    "Suppose halfway through the season he has been up to bat 300 times, hitting 150 out of those times. Let's plot!\n",
    "```\n",
    "data = stats.beta.rvs(81 + 150, 219 + 150, loc = 0, scale = 1, size=1000)\n",
    "plt.hist(data,bins='auto',normed=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the curve is now both *thinner* (0.44 - 0.34 = 0.10 versus 0.34 - 0.20 = 0.14) and *shifted* to the right (higher batting average) than it used to be: We have a better sense of what the player's batting average is!\n",
    "\n",
    "The Beta distribution is best for representing a probabilistic distribution of probabilities- the case where we don't know what a probability is in advance, but we have reasonable guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reparametrization of the Beta\n",
    "We reparametrize $\\alpha$ and $\\beta$ into $\\omega$ and $\\kappa$:\n",
    "```\n",
    "omega = 2\n",
    "kappa = 1\n",
    "alpha, beta = omega*(kappa - 1) + 1, (1 - omega)*(kappa - 2) + 1\n",
    "alpha, beta\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# prior parameters\n",
    "omega = 0.5\n",
    "kappa = 100\n",
    "alpha, beta = omega*(kappa - 1) + 1, (1 - omega)*(kappa - 2) + 1\n",
    "\n",
    "# calculate posterior distribution, using a beta distribution\n",
    "variates = stats.beta(alpha, beta)\n",
    "\n",
    "# beta distribution\n",
    "xs = np.linspace(0, 1, num=1000)\n",
    "pdf = variates.pdf(xs)\n",
    "plt.plot(pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('pdf')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for high values of kappa, we have high certainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior parameters\n",
    "omega = 0.5\n",
    "kappa = 10\n",
    "alpha, beta = omega*(kappa - 1) + 1, (1 - omega)*(kappa - 2) + 1\n",
    "\n",
    "# calculate posterior distribution, using a beta distribution\n",
    "variates = stats.beta(alpha, beta)\n",
    "\n",
    "# beta distribution\n",
    "xs = np.linspace(0, 1, num=1000)\n",
    "pdf = variates.pdf(xs)\n",
    "plt.plot(pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('pdf')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lower values of kappa, not so sure anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior parameters\n",
    "omega = 0.5\n",
    "kappa = 4\n",
    "alpha, beta = omega*(kappa - 1) + 1, (1 - omega)*(kappa - 2) + 1\n",
    "\n",
    "# calculate posterior distribution, using a beta distribution\n",
    "variates = stats.beta(alpha, beta)\n",
    "\n",
    "# beta distribution\n",
    "xs = np.linspace(0, 1, num=1000)\n",
    "pdf = variates.pdf(xs)\n",
    "plt.plot(pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('pdf')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for even lower values of kappa, strange things happen: We get a bias in one direction and we observe more and more one-sided results, eventually down to an absolute certainty!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior parameters\n",
    "omega = 0.5\n",
    "kappa = 3\n",
    "alpha, beta = omega*(kappa - 1) + 1, (1 - omega)*(kappa - 2) + 1\n",
    "\n",
    "# calculate posterior distribution, using a beta distribution\n",
    "variates = stats.beta(alpha, beta)\n",
    "\n",
    "# beta distribution\n",
    "xs = np.linspace(0, 1, num=1000)\n",
    "pdf = variates.pdf(xs)\n",
    "plt.plot(pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('pdf')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior parameters\n",
    "omega = 0.5\n",
    "kappa = 2.5\n",
    "alpha, beta = omega*(kappa - 1) + 1, (1 - omega)*(kappa - 2) + 1\n",
    "\n",
    "# calculate posterior distribution, using a beta distribution\n",
    "variates = stats.beta(alpha, beta)\n",
    "\n",
    "# beta distribution\n",
    "xs = np.linspace(0, 1, num=1000)\n",
    "pdf = variates.pdf(xs)\n",
    "plt.plot(pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('pdf')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior parameters\n",
    "omega = 0.5\n",
    "kappa = 2.05\n",
    "alpha, beta = omega*(kappa - 1) + 1, (1 - omega)*(kappa - 2) + 1\n",
    "\n",
    "# calculate posterior distribution, using a beta distribution\n",
    "variates = stats.beta(alpha, beta)\n",
    "\n",
    "# beta distribution\n",
    "xs = np.linspace(0, 1, num=1000)\n",
    "pdf = variates.pdf(xs)\n",
    "plt.plot(pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('pdf')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior parameters\n",
    "omega = 0.5\n",
    "kappa = 2\n",
    "alpha, beta = omega*(kappa - 1) + 1, (1 - omega)*(kappa - 2) + 1\n",
    "\n",
    "# calculate posterior distribution, using a beta distribution\n",
    "variates = stats.beta(alpha, beta)\n",
    "\n",
    "# beta distribution\n",
    "xs = np.linspace(0, 1, num=1000)\n",
    "pdf = variates.pdf(xs)\n",
    "plt.plot(pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('pdf')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior parameters\n",
    "omega = 0.5\n",
    "kappa = 1.9\n",
    "alpha, beta = omega*(kappa - 1) + 1, (1 - omega)*(kappa - 2) + 1\n",
    "\n",
    "# calculate posterior distribution, using a beta distribution\n",
    "variates = stats.beta(alpha, beta)\n",
    "\n",
    "# beta distribution\n",
    "xs = np.linspace(0, 1, num=1000)\n",
    "pdf = variates.pdf(xs)\n",
    "plt.plot(pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('pdf')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior parameters\n",
    "omega = 0.5\n",
    "kappa = 1.5\n",
    "alpha, beta = omega*(kappa - 1) + 1, (1 - omega)*(kappa - 2) + 1\n",
    "\n",
    "# calculate posterior distribution, using a beta distribution\n",
    "variates = stats.beta(alpha, beta)\n",
    "\n",
    "# beta distribution\n",
    "xs = np.linspace(0, 1, num=1000)\n",
    "pdf = variates.pdf(xs)\n",
    "plt.plot(pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('pdf')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior parameters\n",
    "omega = 0.5\n",
    "kappa = 1\n",
    "alpha, beta = omega*(kappa - 1) + 1, (1 - omega)*(kappa - 2) + 1\n",
    "\n",
    "# calculate posterior distribution, using a beta distribution\n",
    "variates = stats.beta(alpha, beta)\n",
    "\n",
    "# beta distribution\n",
    "xs = np.linspace(0, 1, num=1000)\n",
    "pdf = variates.pdf(xs)\n",
    "plt.plot(pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('pdf')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior parameters\n",
    "omega = 0.5\n",
    "kappa = 0.01\n",
    "alpha, beta = omega*(kappa - 1) + 1, (1 - omega)*(kappa - 2) + 1\n",
    "\n",
    "# calculate posterior distribution, using a beta distribution\n",
    "variates = stats.beta(alpha, beta)\n",
    "\n",
    "# beta distribution\n",
    "xs = np.linspace(0, 1, num=1000)\n",
    "pdf = variates.pdf(xs)\n",
    "plt.plot(pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('pdf')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting reparametrization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Single hierarchy Model\n",
    "We assume that each player has a different hitting average, but we shrink that to a league-average model.\n",
    "\n",
    "Since we have a count of `Hits`` and a number of `AtBats`, the **binomial** is our optimal data likelihood pdf for our dataset.\n",
    "\n",
    "We use the same omega/kappa decomposition we used above:\n",
    "```\n",
    "with pm.Model() as single_hierarchy_model:\n",
    "    # hyperpriors\n",
    "    omega = pm.Beta('omega', 1., 1.)\n",
    "    kappa_minus2 = pm.Gamma('kappa_minus2', 0.01, 0.01)\n",
    "    kappa = pm.Deterministic('kappa', kappa_minus2 + 2)\n",
    "    \n",
    "    theta = pm.Beta('theta', alpha=omega*(kappa-2)+1, beta=(1-omega)*(kappa-2)+1, shape=num_players)\n",
    "    y = pm.Binomial('y', n=df['AtBats'], p=theta, observed=df['Hits'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run:\n",
    "```\n",
    "with single_hierarchy_model:\n",
    "    single_hierarchy_trace = pm.sample(cores=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting [energy transition distribution and marginal energy distribution](https://arviz-devs.github.io/arviz/api/generated/arviz.plot_energy.html) in Hamiltonian Monte Carlo algorithms may help diagnose poor exploration of state space:\n",
    "```\n",
    "az.plot_energy(single_hierarchy_trace)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pm.model_to_graphviz(single_hierarchy_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a way to compare all players with each other:\n",
    "```\n",
    "az.plot_forest(single_hierarchy_trace, var_names=['theta'], combined=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Hierarchy Model\n",
    "Now, we assume that positions are relevant, and that each position has distinct underlying statistics that every player inherits from.\n",
    "```\n",
    "num_positions = df['PriPosNumber'].nunique()\n",
    "position_idx = df['PriPosNumber']\n",
    "num_players = df['PlayerNumber'].nunique()\n",
    "player_idx = df['PlayerNumber']\n",
    "\n",
    "position_idx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 9 different positions in baseball:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(position_idx.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give them an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      6\n",
       "2      3\n",
       "3      3\n",
       "4      2\n",
       "      ..\n",
       "943    0\n",
       "944    4\n",
       "945    0\n",
       "946    0\n",
       "947    8\n",
       "Name: PriPosNumber, Length: 948, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_idx2 = df['PriPosNumber'] - 1\n",
    "position_idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(position_idx2.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up our simulation:\n",
    "```\n",
    "with pm.Model() as double_hierarchy_model:\n",
    "    # hyper-hyperpriors\n",
    "    omega = pm.Beta('omega', 1., 1.)\n",
    "    kappa_minus2 = pm.Gamma('kappa_minus2', 0.01, 0.01)\n",
    "    kappa = pm.Deterministic('kappa', kappa_minus2 + 2)\n",
    "\n",
    "    # Parameters for positions (hyperpriors)\n",
    "    omega_c = pm.Beta('omega_c',\n",
    "                       omega*(kappa-2)+1, (1-omega)*(kappa-2)+1,\n",
    "                       shape = num_positions)    \n",
    "    kappa_c_minus2 = pm.Gamma('kappa_c_minus2',\n",
    "                              0.01, 0.01,\n",
    "                              shape = num_positions)\n",
    "    kappa_c = pm.Deterministic('kappa_c', kappa_c_minus2 + 2)\n",
    "    \n",
    "    # Parameters for players (priors)\n",
    "    #theta = pm.Beta('theta', alpha=omega*(kappa-2)+1, beta=(1-omega)*(kappa-2)+1, shape=num_players)\n",
    "    theta = pm.Beta('theta',\n",
    "                     omega_c[position_idx2]*(kappa_c[position_idx2]-2)+1,\n",
    "                    (1-omega_c[position_idx2])*(kappa_c[position_idx2]-2)+1,\n",
    "                     shape = num_players)\n",
    "    \n",
    "    y = pm.Binomial('y', n=df['AtBats'], p=theta, observed=df['Hits'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize:\n",
    "```\n",
    "pm.model_to_graphviz(double_hierarchy_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate:\n",
    "```\n",
    "with double_hierarchy_model:\n",
    "    double_hierarchy_trace = pm.sample(cores=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the energy of the Hamiltonian to see if we converged:\n",
    "```\n",
    "az.plot_energy(double_hierarchy_trace)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm..... Let's rerun with more iterations:\n",
    "```\n",
    "with double_hierarchy_model:\n",
    "    double_hierarchy_trace2 = pm.sample(2000, cores=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "az.plot_energy(double_hierarchy_trace2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much improvement :-("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forest plot:\n",
    "```\n",
    "az.plot_forest(double_hierarchy_trace, var_names=['omega_c'], combined=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can confirm that pitchers ***suck*** big-time compared to batters!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/minions-laughing.gif\" width=500 />\n",
    "</center>\n",
    "\n",
    "Higher values of kappa imply that batting averages are more concentrated for the category, whereas lower values imply bigger deviations and the HDI also showcases uncertainties. \n",
    "```\n",
    "az.plot_forest(double_hierarchy_trace, var_names=['kappa_c'], combined=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (optional)\n",
    "Can you think of another data model that you can introduce hierarchies in order to simulate less-derived categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "The [puppy dog book](https://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884)!\n",
    "\n",
    "[Doing Bayesian Data Analysis, Second Edition:\n",
    "A Tutorial with R, JAGS, and Stan](https://jkkweb.sitehost.iu.edu/DoingBayesianDataAnalysis/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
